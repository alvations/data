{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from lazyme import per_section, deduplicate, find_files\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_nltk_data = nltk.data.path[0]\n",
    "\n",
    "new_nltk_data = \"packages/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC corpus.\n",
    "directory = new_nltk_data+'/corpora/abc/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with io.open(old_nltk_data+'/corpora/abc/rural.txt') as fin:\n",
    "    rural_texts = [line.strip() for line in fin if line.strip()]\n",
    "with io.open(old_nltk_data+'/corpora/abc/science.txt', encoding='latin_1') as fin:\n",
    "    science_texts = [line.strip().encode('utf8').decode('utf8') for line in fin if \n",
    "                    line.strip().encode('utf8').decode('utf8')]\n",
    "\n",
    "rural_df = pd.DataFrame({'text':rural_texts})\n",
    "rural_df['subcorpora'] = 'Rural News'\n",
    "\n",
    "science_df = pd.DataFrame({'text':science_texts})\n",
    "science_df['subcorpora'] = 'Science News'\n",
    "\n",
    "df_abc = pd.concat([rural_df, science_df])\n",
    "df_abc.to_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', index=False)\n",
    "df_abc = pd.read_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'subcorpora':str})\n",
    "\n",
    "abc_meta = {'title':'Australian Broadcasting Commission 2006',\n",
    "            'source': 'http://www.abc.net.au/',\n",
    "            'subcorpora': {'Rural News': {'source': 'http://www.abc.net.au/rural/news/'},\n",
    "                           'Science News': {'source': 'http://www.abc.net.au/science/news/'}\n",
    "                          },\n",
    "             'xml': {'id':'abc', 'name':\"Australian Broadcasting Commission 2006\",\n",
    "                     'webpage':\"http://www.abc.net.au/\", 'author':\"Australian Broadcasting Commission\",\n",
    "                      'unzip':\"1\"}\n",
    "           }\n",
    "\n",
    "\n",
    "abc_xml = ET.Element(\"package\", id=\"abc\", name=\"Australian Broadcasting Commission 2006\",\n",
    "                  webpage=\"http://www.abc.net.au/\", author=\"Australian Broadcasting Commission\",\n",
    "                  unzip=\"1\")\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(new_nltk_data+'/corpora/abc/abc.xml')\n",
    "\n",
    "with open(new_nltk_data+'/corpora/abc/abc-meta.json', 'w') as fout:\n",
    "    json.dump(abc_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subcorpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PM denies knowledge of AWB kickbacks</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Prime Minister has denied he knew AWB was ...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Letters from John Howard and Deputy Prime Mini...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In one of the letters Mr Howard asks AWB manag...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Opposition's Gavan O'Connor says the lette...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  subcorpora\n",
       "0               PM denies knowledge of AWB kickbacks  Rural News\n",
       "1  The Prime Minister has denied he knew AWB was ...  Rural News\n",
       "2  Letters from John Howard and Deputy Prime Mini...  Rural News\n",
       "3  In one of the letters Mr Howard asks AWB manag...  Rural News\n",
       "4  The Opposition's Gavan O'Connor says the lette...  Rural News"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown\n",
    "\n",
    "directory = new_nltk_data+'/corpora/brown/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "    \n",
    "with open(old_nltk_data+'/corpora/brown/cats.txt') as fin:\n",
    "     categories = {line.strip().split(' ')[0]:line.strip().split(' ')[1] \n",
    "                   for line in fin}\n",
    "        \n",
    "brown_dir = old_nltk_data+'/corpora/brown/'\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(brown_dir):\n",
    "    if filename in ['CONTENTS', 'cats.txt', 'README']:\n",
    "        continue\n",
    "    cat = categories[filename]\n",
    "    with open(brown_dir+filename) as fin:\n",
    "        i = -1\n",
    "        for paragraph in fin.read().split('\\n\\n'):\n",
    "            if not paragraph.strip():\n",
    "                continue\n",
    "            i += 1\n",
    "            j = -1\n",
    "            for sent in paragraph.split('\\n'):\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                j += 1\n",
    "                raw = sent.strip()\n",
    "                text, pos = zip(*[word.split('/') for word in raw.split()])\n",
    "                rows.append({'filename': filename, \n",
    "                              'para_id': i, \n",
    "                              'sent_id': j, \n",
    "                              'raw_text': raw, \n",
    "                              'tokenized_text': ' '.join(text), \n",
    "                              'tokenized_pos': ' '.join(pos), \n",
    "                              'label': cat})\n",
    "                \n",
    "                \n",
    "df_brown = pd.DataFrame(rows)[['filename', 'para_id', 'sent_id', \n",
    "                              'raw_text', 'tokenized_text', 'tokenized_pos', 'label']]\n",
    "df_brown.to_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', index=False)\n",
    "\n",
    "df_brown = pd.read_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', \n",
    "                     dtype={'filename':str, 'para_id':int, 'sent_id':int,\n",
    "                             'raw_text':str, 'tokenized_text':str, 'tokenized_pos':str,\n",
    "                           'label':str})\n",
    "\n",
    "df_brown_cats = df_brown[['filename', 'label']].drop_duplicates().sort_values('filename')\n",
    "df_brown_cats.to_csv(new_nltk_data+'/corpora/brown/cats.tsv', sep='\\t', index=False)\n",
    "\n",
    "brown_readme = \"\"\"BROWN CORPUS\n",
    "\n",
    "A Standard Corpus of Present-Day Edited American\n",
    "English, for use with Digital Computers.\n",
    "\n",
    "by W. N. Francis and H. Kucera (1964)\n",
    "Department of Linguistics, Brown University\n",
    "Providence, Rhode Island, USA\n",
    "\n",
    "Revised 1971, Revised and Amplified 1979\n",
    "\n",
    "http://www.hit.uib.no/icame/brown/bcm.html\n",
    "\n",
    "Distributed with the permission of the copyright holder,\n",
    "redistribution permitted.\"\"\"\n",
    "\n",
    "brown_meta = {'title':'Brown Corpus',\n",
    "              'description': str('A Standard Corpus of Present-Day Edited American English, '\n",
    "                               'for use with Digital Computers.'),\n",
    "              'authors': 'W. N. Francis and H. Kucera (1964)',\n",
    "              'url': 'http://www.hit.uib.no/icame/brown/bcm.html',\n",
    "              'readme': brown_readme}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/brown/brown-meta.json', 'w') as fout:\n",
    "    json.dump(brown_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore/rb ,/, as/cs an/at encouragement/n...</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "      <td>rb , cs at nn in nn nn , pps rb bez jj to vb c...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The/at Unitarian/jj clergy/nns were/bed an/at ...</td>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "      <td>at jj nns bed at jj nn in vbn nns -- cs at nn ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Ezra/np Stiles/np Gannett/np ,/, an/at honorab...</td>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "      <td>np np np , at jj nn in at nn , vbd ppl rb in a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Even/rb so/rb ,/, Gannett/np judiciously/rb ar...</td>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "      <td>rb rb , np rb vbd , at nn-tl md rb vb cs np ``...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>We/ppss today/nr are/ber not/* entitled/vbn to...</td>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "      <td>ppss nr ber * vbn to vb jj nns wps vbd np to b...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  para_id  sent_id  \\\n",
       "0     cd05        0        0   \n",
       "1     cd05        0        1   \n",
       "2     cd05        0        2   \n",
       "3     cd05        0        3   \n",
       "4     cd05        0        4   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  Furthermore/rb ,/, as/cs an/at encouragement/n...   \n",
       "1  The/at Unitarian/jj clergy/nns were/bed an/at ...   \n",
       "2  Ezra/np Stiles/np Gannett/np ,/, an/at honorab...   \n",
       "3  Even/rb so/rb ,/, Gannett/np judiciously/rb ar...   \n",
       "4  We/ppss today/nr are/ber not/* entitled/vbn to...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  Furthermore , as an encouragement to revisioni...   \n",
       "1  The Unitarian clergy were an exclusive club of...   \n",
       "2  Ezra Stiles Gannett , an honorable representat...   \n",
       "3  Even so , Gannett judiciously argued , the Ass...   \n",
       "4  We today are not entitled to excoriate honest ...   \n",
       "\n",
       "                                       tokenized_pos     label  \n",
       "0  rb , cs at nn in nn nn , pps rb bez jj to vb c...  religion  \n",
       "1  at jj nns bed at jj nn in vbn nns -- cs at nn ...  religion  \n",
       "2  np np np , at jj nn in at nn , vbd ppl rb in a...  religion  \n",
       "3  rb rb , np rb vbd , at nn-tl md rb vb cs np ``...  religion  \n",
       "4  ppss nr ber * vbn to vb jj nns wps vbd np to b...  religion  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazetteers\n",
    "\n",
    "directory = new_nltk_data+'/corpora/gazetteers/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(old_nltk_data+'/corpora/gazetteers/'):\n",
    "    if filename in ['LICENSE.txt']:\n",
    "        continue\n",
    "    label = gazetteers_filename2labels[filename]\n",
    "    with io.open(old_nltk_data+'/corpora/gazetteers/'+filename, encoding='ISO-8859-2') as fin:\n",
    "        for line in fin:\n",
    "            if line.strip():\n",
    "                text = line.strip()\n",
    "                if text == 'QuerĂŠtaro':\n",
    "                    text = 'Querétaro'\n",
    "                rows.append({'text':text, 'label':label})\n",
    "\n",
    "df_gazetteers = pd.DataFrame(rows)[['text', 'label']]\n",
    "\n",
    "#alpabet = list('abcdefghijklmnopqrstuvwxyz. ()-,') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "#alpabet += [\"'\"]\n",
    "#[word for word in df_gazetteers['text'] if any(ch for ch in word if ch not in alpabet)]\n",
    "\n",
    "df_gazetteers.to_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', index=False)\n",
    "df_gazetteers = pd.read_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'label':str})\n",
    "\n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "gazetteers_meta = {'title':'Geolocation Gazeteers',\n",
    "                    'subcorpora': {'Mexico States': {'original_file': 'mexstates.txt'},\n",
    "                                   'Canada Provinces': {'original_file': 'caprovinces.txt'},\n",
    "                                   'US State Abbreviations': {'original_file': 'usstates.txt'},\n",
    "                                   'US States': {'original_file': 'usstateabbrev.txt'},\n",
    "                                   'US Cities': {'original_file':'uscities.txt',\n",
    "                                                 'source': 'http://en.wikipedia.org/wiki/List_of_cities_in_the_United_States_with_over_100%2C000_people',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries': {'original_file':'countries.txt',\n",
    "                                                 'source':'http://en.wikipedia.org/wiki/List_of_countries',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries ISO codes': {'original_file': 'isocountries.txt',\n",
    "                                                          'source': 'http://www.guavastudios.com/country-list.htm'\n",
    "                                                          },\n",
    "                                   'Nationalities': {'original_file': 'nationalities.txt',\n",
    "                                                    'source': 'http://www.guavastudios.com/nationalities-list.htm'\n",
    "                                                    },\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/gazetteers/gazetteers-meta.json', 'w') as fout:\n",
    "    json.dump(gazetteers_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "directory = new_nltk_data+'/corpora/words/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en') as fin:\n",
    "    for line in fin:\n",
    "        en_words.append(line.strip())\n",
    "\n",
    "basic_en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en-basic') as fin:\n",
    "    for line in fin:\n",
    "        basic_en_words.append(line.strip())\n",
    "        \n",
    "words_meta = {'title':'Word Lists',\n",
    "              'subcorpora': {'Unix Words':{'source':'http://en.wikipedia.org/wiki/Words_(Unix)'},\n",
    "                           'Ogden Basic English': {'title': 'The ABC of Basic English',\n",
    "                                                   'author':'C.K. Ogden (1932)'}\n",
    "                          }\n",
    "            }\n",
    "\n",
    "unix_words = pd.DataFrame({'text':en_words})\n",
    "ogden_words = pd.DataFrame({'text':basic_en_words})\n",
    "\n",
    "unix_words.to_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', index=False)\n",
    "ogden_words.to_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', index=False)\n",
    "\n",
    "unix_words = pd.read_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', dtype={'text':str})\n",
    "ogden_words = pd.read_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', dtype={'text':str})\n",
    "\n",
    "with open(new_nltk_data+'/corpora/words/words-meta.json', 'w') as fout:\n",
    "    json.dump(words_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = new_nltk_data+'/corpora/movie_reviews/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/pos/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "    \n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/pos/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'pos'\n",
    "                            })\n",
    "                \n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/neg/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "\n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/neg/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'neg'\n",
    "                            })\n",
    "                \n",
    "df_movie_reivews = pd.DataFrame(rows)[['fold_id', 'cv_tag', 'html_id', 'sent_id', 'text', 'tag']]\n",
    "\n",
    "df_movie_reivews.to_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', index=False)\n",
    "df_movie_reivews = pd.read_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', \n",
    "                     dtype={'fold_id':int, 'cv_tag':str, 'html_id':str, 'sent_id':int,\n",
    "                            'text':str, 'tag':str})\n",
    "\n",
    "\n",
    "mr_bibtext = \"\"\"@InProceedings{Pang+Lee:04a,\n",
    "  author =       {Bo Pang and Lillian Lee},\n",
    "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
    "  booktitle =    \"Proceedings of the ACL\",\n",
    "  year =         2004\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "mr_meta = {'title': 'Sentiment Polarity Dataset Version 2.0',\n",
    "           'aka': 'Moview Review Data',\n",
    "           'source': 'http://www.cs.cornell.edu/people/pabo/movie-review-data/',\n",
    "           'authors': 'Bo Pang and Lillian Lee',\n",
    "           'license': 'Distributed with NLTK with permission from the authors.',\n",
    "            'bibtex':mr_bibtext}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/movie_reviews/movie_reviews-meta.json', 'w') as fout:\n",
    "    json.dump(mr_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/webtext/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/webtext/')):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    subcorp = filename.split('.')[0]\n",
    "    with open(old_nltk_data+'/corpora/webtext/' + filename, encoding='latin-1') as fin:\n",
    "        for line in fin:\n",
    "            rows.append({'text': line.strip(), 'domain': subcorp})\n",
    "df_webtext = pd.DataFrame(rows)[['text', 'domain']]\n",
    "\n",
    "\n",
    "df_webtext.to_csv(new_nltk_data +'/corpora/webtext/webtext.tsv', sep='\\t', index=False)\n",
    "df_webtext = pd.read_csv(new_nltk_data + '/corpora/webtext/webtext.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'domain':str})\n",
    "\n",
    "\n",
    "\n",
    "webtext_meta = {'title':'Web Text Corpus',\n",
    "                   'description': str(\"This is a collection of diverse, contemporary text genres, \"\n",
    "                                      \"collected by scraping publicly accessible archives of web postings. \"\n",
    "                                      \"This data is disseminated in preference to publishing URLs for \"\n",
    "                                       \"individuals to download and clean up (the usual model for web corpora).\"),\n",
    "                   \n",
    "                    'subcorpora': {'firefox': {'original_file': 'firefox.txt', \n",
    "                                               'description': 'Firefox support forum'},\n",
    "                                   'overheard': {'original_file': 'overheard.txt', \n",
    "                                               'description': 'Overheard in New York (partly censored)', \n",
    "                                                'source': 'http://www.overheardinnewyork.com/', \n",
    "                                                'year': '2006'},\n",
    "                                   'pirate': {'original_file': 'pirate.txt', \n",
    "                                               'description': \"Movie script from Pirates of the Caribbean: Dead Man's Chest\",\n",
    "                                                'source': 'http://www.overheardinnewyork.com/', \n",
    "                                                'year': '2006'},\n",
    "                                   'grail': {'original_file': 'grail.txt', \n",
    "                                               'description': 'Movie script from Monty Python and the Holy Grail',\n",
    "                                                'source': 'http://www.textfiles.com/media/SCRIPTS/grail', \n",
    "                                                'year': '2006'},\n",
    "                                   'singles': {'original_file': 'singles.txt', \n",
    "                                               'description': 'Singles ads',\n",
    "                                                'source': 'http://search.classifieds.news.com.au/',},\n",
    "                                   'wine': {'original_file': 'wine.txt', \n",
    "                                               'description': 'Fine Wine Diary',\n",
    "                                                'source': 'http://www.finewinediary.com/', \n",
    "                                                'year': '2005-6'},\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/webtext/webtext-meta.json', 'w') as fout:\n",
    "    json.dump(webtext_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/names/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "\n",
    "rows =[]\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/names/')):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    label = filename.split('.')[0]\n",
    "    with open(old_nltk_data+'/corpora/names/' + filename) as fin:\n",
    "        for line in fin:\n",
    "            rows.append({'text': line.strip(), 'gender': label})\n",
    "        \n",
    "df_names = pd.DataFrame(rows)[['text', 'gender']]\n",
    "df_names.to_csv(new_nltk_data+'/corpora/names/names.tsv', sep='\\t', index=False)\n",
    "df_names = pd.read_csv(new_nltk_data+'/corpora/names/names.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'gender':str})\n",
    "            \n",
    "names_readme = \"\"\"Names Corpus, Version 1.3 (1994-03-29)\n",
    "Copyright (C) 1991 Mark Kantrowitz\n",
    "Additions by Bill Ross\n",
    "\n",
    "This corpus contains 5001 female names and 2943 male names, sorted\n",
    "alphabetically, one per line.\n",
    "\n",
    "You may use the lists of names for any purpose, so long as credit is\n",
    "given in any published work. You may also redistribute the list if you\n",
    "provide the recipients with a copy of this README file. The lists are\n",
    "not in the public domain (I retain the copyright on the lists) but are\n",
    "freely redistributable.  If you have any additions to the lists of\n",
    "names, I would appreciate receiving them.\n",
    "\n",
    "Mark Kantrowitz <mkant+@cs.cmu.edu>\n",
    "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\"\"\"\n",
    "    \n",
    "names_meta = {'title': 'Names Corpus',\n",
    "              'version': '1.3 (1994-03-29)',\n",
    "              'description': 'This corpus contains 5001 female names and 2943 male names',\n",
    "              'authors': 'Mark Kantrowitz, Bill Ross',\n",
    "              'license': 'Copyright (C) 1991 Mark Kantrowitz',\n",
    "              'source': 'http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/',\n",
    "              'readme': names_readme}\n",
    "\n",
    "\n",
    "with open(new_nltk_data+'/corpora/names/names-meta.json', 'w') as fout:\n",
    "    json.dump(names_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the Union\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "all_sotu = {}\n",
    "\n",
    "text_url = 'http://stateoftheunion.onetwothree.net/texts/'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "for li in BeautifulSoup(requests.get(text_url + 'index.html').content).find_all('li'):\n",
    "    if not li.find('a')['href']:\n",
    "        continue\n",
    "\n",
    "    sotu = text_url + li.find('a')['href']\n",
    "    if sotu.split('/')[-1].split('.')[0].isdigit():\n",
    "        year = li.find('a').text.split(', ')[-1]\n",
    "        if year in all_sotu:\n",
    "            continue\n",
    "        else:\n",
    "            soup = BeautifulSoup(requests.get(sotu, headers=headers).content)\n",
    "            name = soup.find('h2').text\n",
    "            date = soup.find('h3').text\n",
    "            year = date.split(', ')[1]\n",
    "            print(year, end=', ')\n",
    "            texts = [str(p) for p in soup.find_all('p')]\n",
    "            all_sotu[year] = {'year':year, 'date':date, 'name':name, 'texts':'\\n\\n'.join(texts).strip()}\n",
    "            \n",
    "for year in all_sotu:\n",
    "    lastname = all_sotu[year]['name'].split()[-1]\n",
    "    with open(f'sotu/{lastname}-{year}', 'w') as fout:\n",
    "        print(all_sotu[year]['texts'].strip(), file=fout)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/state_union/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "df_sotu = pd.read_csv('stateunion.tsv', sep='\\t', \n",
    "                     dtype={'texts':str, 'date':str, 'name':str},\n",
    "                     index_col=0)\n",
    "\n",
    "df_sotu.T['texts'] = ['\\n\\n'.join([deduplicate(' '.join(para.split('\\n')), ' ') \n",
    "                       for para in re.sub('<[^<]+?>', '', raw).strip().split('\\n\\n')])\n",
    "                      for raw in df_sotu.T.texts]\n",
    "\n",
    "sotu_meta = {'title': 'State of the Union: Addrresses', \n",
    "             'author': 'Brad Borevitz', \n",
    "             'source': 'http://stateoftheunion.onetwothree.net/texts/',\n",
    "             'year': '1790-2018'\n",
    "            }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/state_union/state_union-meta.json', 'w') as fout:\n",
    "    json.dump(sotu_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    \n",
    "df_sotu.to_csv(new_nltk_data+'/corpora/state_union/state_union.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "      <th>1798</th>\n",
       "      <th>1799</th>\n",
       "      <th>...</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>January 8, 1790</td>\n",
       "      <td>October 25, 1791</td>\n",
       "      <td>November 6, 1792</td>\n",
       "      <td>December 3, 1793</td>\n",
       "      <td>November 19, 1794</td>\n",
       "      <td>December 8, 1795</td>\n",
       "      <td>December 7, 1796</td>\n",
       "      <td>November 22, 1797</td>\n",
       "      <td>December 8, 1798</td>\n",
       "      <td>December 3, 1799</td>\n",
       "      <td>...</td>\n",
       "      <td>February 24, 2009</td>\n",
       "      <td>January 27, 2010</td>\n",
       "      <td>January 25, 2011</td>\n",
       "      <td>January 24, 2012</td>\n",
       "      <td>February 12, 2013</td>\n",
       "      <td>January 28, 2014</td>\n",
       "      <td>January 20, 2015</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>February 28, 2017</td>\n",
       "      <td>January 30, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>...</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "      <td>Madame Speaker, Vice President Biden, Members ...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1790  \\\n",
       "date                                     January 8, 1790   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1791  \\\n",
       "date                                    October 25, 1791   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1792  \\\n",
       "date                                    November 6, 1792   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1793  \\\n",
       "date                                    December 3, 1793   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1794  \\\n",
       "date                                   November 19, 1794   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1795  \\\n",
       "date                                    December 8, 1795   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1796  \\\n",
       "date                                    December 7, 1796   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1797  \\\n",
       "date                                   November 22, 1797   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1798  \\\n",
       "date                                    December 8, 1798   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1799  \\\n",
       "date                                    December 3, 1799   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                             ...                          \\\n",
       "date                         ...                           \n",
       "name                         ...                           \n",
       "texts                        ...                           \n",
       "\n",
       "                                                    2009  \\\n",
       "date                                   February 24, 2009   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Mr. Vice President, Members of...   \n",
       "\n",
       "                                                    2010  \\\n",
       "date                                    January 27, 2010   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Vice President Biden, Members ...   \n",
       "\n",
       "                                                    2011  \\\n",
       "date                                    January 25, 2011   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2012  \\\n",
       "date                                    January 24, 2012   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2013  \\\n",
       "date                                   February 12, 2013   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2014  \\\n",
       "date                                    January 28, 2014   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2015  \\\n",
       "date                                    January 20, 2015   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2016  \\\n",
       "date                                    January 12, 2016   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2017  \\\n",
       "date                                   February 28, 2017   \n",
       "name                                     Donald J. Trump   \n",
       "texts  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "\n",
       "                                                    2018  \n",
       "date                                    January 30, 2018  \n",
       "name                                     Donald J. Trump  \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sotu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "      <th>1798</th>\n",
       "      <th>1799</th>\n",
       "      <th>...</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>January 8, 1790</td>\n",
       "      <td>October 25, 1791</td>\n",
       "      <td>November 6, 1792</td>\n",
       "      <td>December 3, 1793</td>\n",
       "      <td>November 19, 1794</td>\n",
       "      <td>December 8, 1795</td>\n",
       "      <td>December 7, 1796</td>\n",
       "      <td>November 22, 1797</td>\n",
       "      <td>December 8, 1798</td>\n",
       "      <td>December 3, 1799</td>\n",
       "      <td>...</td>\n",
       "      <td>February 24, 2009</td>\n",
       "      <td>January 27, 2010</td>\n",
       "      <td>January 25, 2011</td>\n",
       "      <td>January 24, 2012</td>\n",
       "      <td>February 12, 2013</td>\n",
       "      <td>January 28, 2014</td>\n",
       "      <td>January 20, 2015</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>February 28, 2017</td>\n",
       "      <td>January 30, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>...</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "      <td>Madame Speaker, Vice President Biden, Members ...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1790  \\\n",
       "date                                     January 8, 1790   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1791  \\\n",
       "date                                    October 25, 1791   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1792  \\\n",
       "date                                    November 6, 1792   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1793  \\\n",
       "date                                    December 3, 1793   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1794  \\\n",
       "date                                   November 19, 1794   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1795  \\\n",
       "date                                    December 8, 1795   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1796  \\\n",
       "date                                    December 7, 1796   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1797  \\\n",
       "date                                   November 22, 1797   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1798  \\\n",
       "date                                    December 8, 1798   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1799  \\\n",
       "date                                    December 3, 1799   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                             ...                          \\\n",
       "date                         ...                           \n",
       "name                         ...                           \n",
       "texts                        ...                           \n",
       "\n",
       "                                                    2009  \\\n",
       "date                                   February 24, 2009   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Mr. Vice President, Members of...   \n",
       "\n",
       "                                                    2010  \\\n",
       "date                                    January 27, 2010   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Vice President Biden, Members ...   \n",
       "\n",
       "                                                    2011  \\\n",
       "date                                    January 25, 2011   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2012  \\\n",
       "date                                    January 24, 2012   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2013  \\\n",
       "date                                   February 12, 2013   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2014  \\\n",
       "date                                    January 28, 2014   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2015  \\\n",
       "date                                    January 20, 2015   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2016  \\\n",
       "date                                    January 12, 2016   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2017  \\\n",
       "date                                   February 28, 2017   \n",
       "name                                     Donald J. Trump   \n",
       "texts  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "\n",
       "                                                    2018  \n",
       "date                                    January 30, 2018  \n",
       "name                                     Donald J. Trump  \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('sotu'):\n",
    "    with open('sotu/'+filename) as fin, open('sotu-clean/'+filename, 'w') as fout:\n",
    "        fout.write(re.sub('<[^<]+?>', '', fin.read()).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City.db\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sem.chat80 import cities2table, sql_query\n",
    "from sqlite3 import OperationalError\n",
    "try:\n",
    "    cities2table('cities.pl', 'city', 'city.db', verbose=True, setup=True)\n",
    "except OperationalError:\n",
    "    pass \n",
    "\n",
    "directory = new_nltk_data+'/corpora/city_database/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with open(directory+'city.tsv', 'w') as fout:\n",
    "    for row in sql_query('corpora/city_database/city.db', \"SELECT * FROM city_table\"):\n",
    "        city, country, population = row\n",
    "        print('\\t'.join([city, country, str(population)]), end='\\n', file=fout)\n",
    "    \n",
    "chat80_meta = {'title': 'Chat80', \n",
    "             'author': '', \n",
    "             'source': '',\n",
    "            }\n",
    "\n",
    "\n",
    "directory = new_nltk_data+'/corpora/chat80/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "# contain.pl\n",
    "with open(directory+'contain.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/contain.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'^contains0\\((.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                country, contains = matches[0]\n",
    "                print('\\t'.join([country, contains]), end='\\n', file=fout)\n",
    "\n",
    "\n",
    "# borders.pl\n",
    "with open(directory+'borders.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/borders.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'borders\\((.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                query, bordering = matches[0]\n",
    "                print('\\t'.join([query, bordering]), end='\\n', file=fout)\n",
    "                \n",
    "# cities.pl\n",
    "with open(directory+'cities.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/cities.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'city\\((.*),(.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                city, country, population = matches[0]\n",
    "                print('\\t'.join([city, country, population]), end='\\n', file=fout)\n",
    "                \n",
    "# countries.pl\n",
    "with open(directory+'countries.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/countries.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'country\\((.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                country, region, latitude, longtitude, area, population, capital, currency = matches[0]\n",
    "                print('\\t'.join(matches[0]), end='\\n', file=fout)\n",
    "\n",
    "# rivers.pl \n",
    "with open(directory+'rivers.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/rivers.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'river\\((.*),\\[(.*)\\]\\)\\.$', line)\n",
    "            if matches:\n",
    "                river, flows_thru = matches[0]\n",
    "                print('\\t'.join([river, str(flows_thru.split(','))]), file=fout, end='\\n')\n",
    "            \n",
    "# world1.pl\n",
    "with open(directory+'world1-circle-of-latitutde.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/world1.pl') as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('circle_of_latitude'):\n",
    "                matches = re.findall(r'(circle_of_latitude)\\((.*),(.*)\\)\\.', line)\n",
    "                rel, circle, number = matches[0]\n",
    "                print('\\t'.join(['circle_of_latitude', circle, number]), end='\\n', file=fout)\n",
    "                \n",
    "with open(directory+'world1-in-continent.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/world1.pl') as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('in_continent'):\n",
    "                matches = re.findall(r'in_continent\\((.*)\\,(.*)\\)\\.', line)\n",
    "                region, continent  = matches[0]\n",
    "                print('\\t'.join(['in_continent', region, continent]), end='\\n', file=fout)\n",
    "                \n",
    "with open(directory+'world1-continent-ocean-sea.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/world1.pl') as fin:\n",
    "        for line in fin:\n",
    "            if not line.startswith('in_continent'):\n",
    "                matches = re.findall(r'(continent|ocean|sea)\\((.*)\\)\\.', line)\n",
    "                if matches:\n",
    "                    rel, entity = matches[0]\n",
    "                    print('\\t'.join([rel, entity]), end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dolch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyme import find_files\n",
    "\n",
    "dolch_meta = {'title':'Dolch Word List',\n",
    "            'description': str(\n",
    "                           \"This corpus contains a list of frequently used English words, grouped according to their part of speech.\"\n",
    "                           \"These are 220 sight words that make up most of children's reading materials.\"),\n",
    "            'cite': 'Dolch, E. W. (1936). A basic sight vocabulary. The Elementary School Journal, 36(6), 456--460.',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/dolch/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(directory+'dolch.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['pos', 'word']), end='\\n', file=fout)\n",
    "    for filename in find_files(old_nltk_data+'/corpora/dolch/', '*'):\n",
    "        if not filename.lower().endswith('readme'):\n",
    "            with open(filename) as fin:\n",
    "                pos = filename.split('/')[-1]\n",
    "                for line in fin:\n",
    "                    print('\\t'.join([pos, line.strip()]), end='\\n', file=fout)\n",
    "\n",
    "with open(new_nltk_data+'/corpora/dolch/dolch-meta.json', 'w') as fout:\n",
    "    json.dump(dolch_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comtrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyme import find_files, per_chunk\n",
    "\n",
    "format_description = \"\"\"The data is in giza++ format, consisting of triples of\n",
    "L1, L2, and alignments, e.g.:\n",
    "\n",
    "English-French:\n",
    "Resumption of the session\n",
    "Reprise de la session\n",
    "0-0 1-1 2-2 3-3\n",
    "\n",
    "German-English:\n",
    "Wiederaufnahme der Sitzungsperiode\n",
    "Resumption of the session\n",
    "0-0 1-1 1-2 2-3\n",
    "\n",
    "German-French:\n",
    "Wiederaufnahme der Sitzungsperiode\n",
    "Reprise de la session\n",
    "0-0 1-1 1-2 2-3\"\"\"\n",
    "\n",
    "comtrans_meta = {'title':'COMTRANS Corpus Sample',\n",
    "            'description': str(\n",
    "                           \"3.3% of the COMTRANS data, distributed with permission.\\n\\n\"\n",
    "                           )+format_description,\n",
    "            'authors': 'Reinhard Rapp',\n",
    "            'source': 'http://www.fask.uni-mainz.de/user/rapp/comtrans/',\n",
    "            'cite': '',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/comtrans/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/comtrans/comtrans-meta.json', 'w') as fout:\n",
    "    json.dump(comtrans_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + 'comtrans-sample.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['filename', \n",
    "                     'src_lang', 'trg_lang', 'idx', \n",
    "                    'src', 'trg', 'alignment']), file=fout, end='\\n')\n",
    "    \n",
    "    for filename in find_files(old_nltk_data + '/corpora/comtrans/', '*.txt'):\n",
    "        x = filename.split('/')[-1].split('.')[0].split('-')\n",
    "        _, src, trg = x\n",
    "        with open(filename, encoding='latin-1') as fin:\n",
    "\n",
    "            for idx, three_lines in enumerate(per_chunk(fin, n=3)):\n",
    "                srcline, trgline, align = three_lines\n",
    "                srcline = srcline.strip().encode('utf-8').decode('utf-8')\n",
    "                trgline = trgline.strip().encode('utf-8').decode('utf-8')\n",
    "                align = align.strip().encode('utf-8').decode('utf-8')\n",
    "                print('\\t'.join([filename.split('/')[-1], \n",
    "                                 src, trg, str(idx), \n",
    "                                 srcline, trgline, align]),\n",
    "                      file=fout, end='\\n'\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + 'comtrans-full.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['filename', \n",
    "                     'src_lang', 'trg_lang', 'idx', \n",
    "                    'src', 'trg', 'alignment']), file=fout, end='\\n')\n",
    "    \n",
    "    for filename in find_files(old_nltk_data + '/corpora/comtrans-full/', '*.txt'):\n",
    "        x = filename.split('/')[-1].split('.')[0].split('-')\n",
    "        _, _, src, trg, _  = x\n",
    "        with open(filename, encoding='latin-1') as fin:\n",
    "\n",
    "            for idx, three_lines in enumerate(per_chunk(fin, n=3)):\n",
    "                srcline, trgline, align = three_lines\n",
    "                srcline = srcline.strip().encode('utf-8').decode('utf-8')\n",
    "                trgline = trgline.strip().encode('utf-8').decode('utf-8')\n",
    "                align = align.strip().encode('utf-8').decode('utf-8')\n",
    "                print('\\t'.join([filename.split('/')[-1], \n",
    "                                 src, trg, str(idx), \n",
    "                                 srcline, trgline, align]),\n",
    "                      file=fout, end='\\n'\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crubadan\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "crubadan_readme = \"\"\"\n",
    "Language Id Corpus\n",
    "Kevin Scannell\n",
    "\n",
    "This directory contains 3-gram frequencies for 449 writing systems \n",
    "gathered by the web crawler \"An Crúbadán\", as of 11 April 2010.\n",
    "See http://borel.slu.edu/crubadan/ for more information.\n",
    "\n",
    "The web crawler works at the level of \"writing systems\" vs. \"languages\",\n",
    "so for example Serbian Cyrillic and Serbian Latin are treated\n",
    "separately, as are Portuguese as spoken in Brazil vs. Portugal, etc.\n",
    "The 3-gram files are named using 2- or 3-letter \"writing system codes\"\n",
    "that were never intended to be exposed to the outside world.\n",
    "We are working on establishing a mapping between our codes and\n",
    "the writing systems laid out in Oliver Streiter's XNL-RDF database.  \n",
    "\n",
    "The file table.txt lists all 449 writing systems.  The first column\n",
    "contains the internal Crúbadán code, the second column contains the\n",
    "ISO 639-3 code for the language represented by the writing system, and\n",
    "the third column is an English language description.\n",
    "\n",
    "Copyright 2010 Kevin P. Scannell <kscanne at gmail dot com>\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "crubadan_meta = {'title':'Crúbadán Language Id Corpus',\n",
    "            'description': str(\n",
    "                            \"This directory contains 3-gram frequencies for 449 writing systems\\n\"\n",
    "                            'gathered by the web crawler \"An Crúbadán\", as of 11 April 2010.\\n'\n",
    "                            \"See http://borel.slu.edu/crubadan/ for more information.\\n\"\n",
    "                           ),\n",
    "            'readme': crubadan_readme,\n",
    "            'authors': 'Kevin Scannell',\n",
    "            'source': 'http://borel.slu.edu/crubadan',\n",
    "            'cite': '',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/crubadan/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/crubadan/crubadan-meta.json', 'w') as fout:\n",
    "    json.dump(crubadan_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    \n",
    "    \n",
    "language_mappings = []\n",
    "iso6392_to_iso6393 = {}\n",
    "with open(old_nltk_data+'/corpora/crubadan/table.txt') as fin:\n",
    "    for line in fin:\n",
    "        iso6392, iso6393, lang = line.strip().split('\\t')\n",
    "        language_mappings.append({'iso639-2':iso6392, 'iso639-3':iso6393, 'language':lang})\n",
    "        iso6392_to_iso6393[iso6392] = iso6393\n",
    "df_lang = pd.DataFrame.from_dict(language_mappings)  \n",
    "\n",
    "df_lang.to_csv(new_nltk_data+'/corpora/crubadan/language_mapping.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "trigram_counter = defaultdict(Counter)\n",
    "for filename in find_files(old_nltk_data+'/corpora/crubadan/', '*3grams.txt'):\n",
    "    lang = iso6392_to_iso6393[filename.split('/')[-1].split('-')[0]]\n",
    "    with open(filename) as fin:\n",
    "        for line in fin:\n",
    "            count, gram = line.strip().split(' ')\n",
    "            trigram_counter[lang][gram] = int(count)\n",
    "            rows.append({'lang':lang, 'trigram':gram, 'count':int(count)})\n",
    "            \n",
    "df_crubadan = pd.DataFrame.from_dict(rows)\n",
    "\n",
    "df_crubadan.to_csv(new_nltk_data +'/corpora/crubadan/crubadan.tsv', sep='\\t', index=False)\n",
    "\n",
    "with open(new_nltk_data +'/corpora/crubadan/crubadan.pkl', 'wb') as fout:\n",
    "    pickle.dump(trigram_counter, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import machado\n",
    "from nltk.data import LazyLoader\n",
    "\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *\n",
    "\n",
    "def _read_para_block(stream):\n",
    "    paras = []\n",
    "    for para in read_blankline_block(stream):\n",
    "        paras.append(\n",
    "            [\n",
    "                sent.replace('\\n', ' ')\n",
    "                for sent in sent_tokenize_pt(para)\n",
    "            ]\n",
    "        )\n",
    "    return paras\n",
    "\n",
    "def machado_paras(fileid):\n",
    "    return concat(\n",
    "                [\n",
    "                    machado.CorpusView(path, _read_para_block, encoding=enc)\n",
    "                    for (path, enc, fileid) in machado.abspaths(fileid, True, True)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "sent_tokenize_pt = LazyLoader(\"tokenizers/punkt/portuguese.pickle\").tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "machado_meta = {'title':'Machado de Assis -- Obra Completa',\n",
    "            'description': str(\n",
    "                            \"This directory contains 3-gram frequencies for 449 writing systems\\n\"\n",
    "                            'gathered by the web crawler \"An Crúbadán\", as of 11 April 2010.\\n'\n",
    "                            \"See http://borel.slu.edu/crubadan/ for more information.\\n\"\n",
    "                           ),\n",
    "            'readme': machado.readme(),\n",
    "            'authors': '',\n",
    "            'source': 'http://machado.mec.gov.br',\n",
    "            'cite': '',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/machado/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/machado/machado-meta.json', 'w') as fout:\n",
    "    json.dump(machado_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:14<00:00,  4.91it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(new_nltk_data +'/corpora/machado/machado.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['genre', 'filename', 'para_idx', 'sent_idx', 'sent']), end='\\n', file=fout)\n",
    "    for filepath in tqdm(machado.fileids()):\n",
    "        genre, filename = filepath.split('/')\n",
    "        for para_idx, para in enumerate(machado_paras(filepath)):\n",
    "            for sent_idx, sent in enumerate(para):\n",
    "                print('\\t'.join(map(str, [genre, filename, para_idx, sent_idx, sent])), end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switchboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import switchboard\n",
    "\n",
    "switchboard_meta = {'title':'Switchboard Corpus Sample',\n",
    "            'description': str('Derived from \"TalkBank Switchboard Corpus, Version 0.1\"'\n",
    "                           ),\n",
    "            'readme': switchboard.readme(),\n",
    "            'authors': 'David Graff & Steven Bird (2000)',\n",
    "            'source': 'http://www.ldc.upenn.edu/Catalog/LDC93S7.html]',\n",
    "            'cite': str(\"David Graff & Steven Bird (2000).  Many uses, many annotations for large \"\n",
    "                        \"speech corpora: Switchboard and TDT as case studies.  Proceedings of the \"\n",
    "                        \"Second International Conference on Language Resources and Evaluation, \"\n",
    "                        \"pp. 427-433, Paris: European Language Resources Association, 2000. \"\n",
    "                        \"http://arXiv.org/abs/cs/0007024\"),\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/switchboard/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/switchboard/switchboard-meta.json', 'w') as fout:\n",
    "    json.dump(switchboard_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_nltk_data +'/corpora/switchboard/switchboard-sample.tsv', 'w') as fout:\n",
    "    for discourse_idx, (discourse, tagged_discourse) in enumerate(zip(switchboard.discourses(),switchboard.tagged_discourses())):\n",
    "        for turn_idx, (turn, tagged_turn) in enumerate(zip(discourse, tagged_discourse)):\n",
    "            text = re.search(f\"^\\<{turn.speaker}\\.{turn.id}\\: (.*)\\>$\", str(turn)).group(1)[1:-1]\n",
    "            tagged_text = re.search(f\"^\\<{turn.speaker}\\.{turn.id}\\: (.*)\\>$\", str(tagged_turn)).group(1)[1:-1]\n",
    "            tags = [token_tag.split('/')[-1] for token_tag in tagged_text.split(' ')]\n",
    "            print('\\t'.join(map(str, [discourse_idx, turn_idx, turn.speaker, turn.id, text, ' '.join(tags)])), \n",
    "                  end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.switchboard.SwitchboardTurn"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<A.1: 'Uh/UH ,/, do/VBP you/PRP have/VB a/DT pet/NN Randy/NNP ?/.'>, <B.2: 'Uh/UH ,/, yeah/UH ,/, currently/RB we/PRP have/VBP a/DT poodle/NN ./.'>, <A.3: 'A/DT poodle/NN ,/, miniature/JJ or/CC ,/, uh/UH ,/, full/JJ size/NN ?/.'>, <B.4: \"Yeah/UH ,/, uh/UH ,/, it/PRP 's/BES ,/, uh/UH miniature/JJ ./.\">, <A.5: 'Uh-huh/UH ./.'>, <B.6: 'Yeah/UH ./.'>, <A.7: 'I/PRP read/VBD somewhere/RB that/IN ,/, the/DT poodles/NNS is/VBZ one/CD of/IN the/DT ,/, the/DT most/RBS intelligent/JJ dogs/NNS ,/, uh/UH ,/, around/RB ./.'>, <B.8: \"Well/UH ,/, um/UH ,/, I/PRP would/MD n't/RB ,/, uh/UH ,/, I/PRP definitely/RB would/MD n't/RB dispute/VB that/IN ,/, it/PRP ,/, it/PRP 's/BES actually/RB my/PRP$ wife/NN 's/POS dog/NN ,/, uh/UH ,/, I/PRP ,/, I/PRP became/VBD part/NN owner/NN six/CD months/NNS ago/RB when/WRB we/PRP got/VBD married/VBN ,/, but/CC ,/, uh/UH ,/, it/PRP ,/, uh/UH ,/, definitely/RB responds/VBZ to/IN ,/, uh/UH ,/, to/IN authority/NN and/CC ,/, I/PRP 've/VBP had/VBN dogs/NNS in/IN the/DT past/JJ and/CC ,/, uh/UH ,/, it/PRP seems/VBZ ,/, it/PRP seems/VBZ to/TO ,/, uh/UH ,/, respond/VB real/RB well/RB ,/, it/PRP ,/, it/PRP she/PRP 's/HVS ,/, she/PRP 's/HVS picked/VBN up/RP a/DT lot/NN of/IN things/NNS ,/, uh/UH ,/, just/RB ,/, just/RB by/IN ,/, uh/UH ,/, teaching/VBG by/IN force/NN ,/, I/PRP guess/VBP is/VBZ what/WP I/PRP 'd/MD like/VB to/TO say/VB ./.\">, <A.9: \"Oh/UH ,/, uh-huh/UH ./. So/RB ,/, you/PRP ,/, you/PRP 've/VBP only/RB known/VBN the/DT dog/NN ,/, wh-/XX ,/, how/WRB long/JJ did/VBD you/PRP say/VB ./.\">, <B.10: 'Well/UH ,/, about/RB a/DT year/NN I/PRP guess/VBP ./.'>, <A.11: 'Oh/UH ,/, well/UH ,/, uh/UH ,/, is/VBZ it/PRP ,/, uh/UH ,/, uh/UH ,/, how/WRB old/JJ is/VBZ the/DT dog/NN ?/.'>, <B.12: 'It/PRP just/RB turned/VBD two/CD ,/, I/PRP believe/VBP ./.'>, <A.13: \"Oh/UH ,/, it/PRP 's/BES still/RB just/RB a/DT pup/NN ./.\">, <B.14: 'Pretty/RB much/JJ ,/, yeah/UH ,/, yeah/UH ./.'>, <A.15: 'Yeah/UH ,/, I/PRP have/VBP a/DT ,/, uh/UH ,/, well/UH a/DT mutt/NN ,/, myself/PRP ./. I/PRP call/VBP it/PRP a/DT ,/, uh/UH ,/, uh/UH ,/, Chowperd/NNP ./.'>, <B.16: 'Okay/UH ./.'>, <A.17: \"It/PRP 's/BES ,/, uh/UH ,/, part/NN Chow/NNP and/CC part/NN Shepherd/NNP and/CC it/PRP ,/, as/IN I/PRP understand/VBP it/PRP ,/, uh/UH ,/, both/DT sides/NNS of/IN the/DT ,/, were/VBD thoroughbreds/NNS ./. So/RB ,/, she/PRP 's/BES a/DT genuine/JJ Chowperd/NNP ./.\">, <B.18: 'Oh/UH ,/, that/DT sounds/VBZ interesting/JJ ./.'>, <A.19: 'She/PRP has/VBZ the/DT ,/, the/DT color/NN and/CC the/DT black/JJ to-/NN ,/, tongue/NN of/IN a/DT Chow/NNP ,/, but/CC ,/, uh/UH ,/, she/PRP has/VBZ the/DT shap-/NN ,/, the/DT shape/NN of/IN the/DT ,/, uh/UH ,/, uh/UH ,/, Shepherd/NNP ./.'>, <B.20: \"Oh/UH ,/, that/DT 's/BES ,/, that/DT 's/BES neat/JJ ./. How/WRB ,/, about/RB how/WRB big/JJ then/RB ?/.\">, <A.21: \"Oh/UH ,/, she/PRP weighs/VBZ in/RP at/IN about/RB fifty/CD pounds/NNS ,/, so/RB she/PRP 's/BES a/DT medium/JJ size/NN ./.\">, <B.22: 'Yeah/UH ,/, yeah/UH ./.'>, <A.23: \"But/CC she/PRP 's/BES big/JJ enough/RB to/TO be/VB intimidating/JJ ,/,\">, <B.24: 'Most/JJS definitely/RB ./.'>, <A.25: 'it/PRP is/VBZ a/DT fi/VBN ,/, fixed/VBN female/NN ,/, by/IN the/DT way/NN ,/,'>, <B.26: 'Yeah/UH ./.'>, <A.27: 'and/CC right/RB from/IN day/NN one/CD ,/, she/PRP was/VBD teaching/VBG me/PRP ./.'>, <B.28: \"Oh/UH ,/, I/PRP would/MD n't/RB doubt/VB it/PRP ,/, yeah/UH ./.\">, <A.29: \"She/PRP 's/BES the/DT most/RBS intelligent/JJ dog/NN I/PRP 've/VBP ever/RB seen/VBN ./. Course/NN ,/, I/PRP 'm/VBP a/DT little/RB prejudiced/JJ ,/, of/IN course/NN ./.\">, <B.30: \"Well/UH that/DT 's/BES understandable/JJ ,/, yeah/UH ,/, it/PRP 's/BES ,/, uh/UH ,/,\">, <A.31: 'You/PRP know/VBP ,/, the/DT first/JJ time/NN I/PRP brought/VBD her/PRP$ home/NN ,/, she/PRP was/VBD only/RB ,/, uh/UH ,/, was/VBD it/PRP six/CD weeks/NNS old/JJ ./. And/CC I/PRP spread/VBD the/DT newspapers/NNS out/RP in/IN the/DT kitchen/NN area/NN ./.'>, <B.32: 'Uh-huh/UH ./.'>, <A.33: 'But/CC ,/, uh/UH ,/, next/JJ morning/NN ,/, she/PRP let/VBD me/PRP know/VB in/IN no/DT uncertain/JJ terms/NNS that/IN she/PRP wanted/VBD to/TO use/VB the/DT bathroom/NN ./.'>, <B.34: 'Okay/UH ./.'>, <A.35: 'So/RB ,/, on/IN next/JJ night/NN ,/, I/PRP spread/VBD the/DT newspaper/NN in/IN the/DT bathroom/NN and/CC she/PRP used/VBD them/PRP there/RB ./.'>, <B.36: 'Oh/UH ./.'>, <A.37: \"But/CC it/PRP was/VBD n't/RB too/RB long/JJ until/IN she/PRP ,/, uh/UH ,/, found/VBD out/RP she/PRP could/MD wait/VB until/IN I/PRP let/VBP her/PRP out/RB in/IN the/DT morning/NN ./.\">, <B.38: 'Yeah/UH ./.'>, <A.39: 'And/CC since/IN then/RB ,/, I/PRP ,/, I/PRP live/VBP alone/RB ,/,'>, <B.40: 'Okay/UH ./.'>, <A.41: \"and/CC ,/, uh/UH ,/, I/PRP live/VBP in/IN motor/NN home/NN ,/, by/IN the/DT way/NN ,/, I/PRP 'm/VBP ,/, uh/UH ,/, an/DT R/NN V/NN ,/, full/GW time/^RB R/NN V/NN -er/NN ,/, and/CC it/PRP 's/BES ,/, it/PRP 's/BES such/PDT a/DT pleasure/NN to/TO come/VB home/NN at/IN night/NN and/CC you/PRP can/MD see/VB her/PRP$ smiling/VBG from/IN ear/NN to/IN ear/NN ,/, she/PRP 's/BES so/RB happy/JJ to/TO see/VB me/PRP ./.\">, <B.42: 'Yeah/UH ,/, definitely/UH ./.'>, <A.43: \"And/CC ,/, uh/UH ,/, I/PRP do/VBP n't/RB know/VB if/IN you/PRP get/VBP that/DT kind/NN of/IN greeting/NN or/CC not/RB ./.\">, <B.44: \"Yeah/UH ,/, I/PRP can/MD honestly/RB say/VB we/PRP do/VBP ,/, uh/UH ,/, we/PRP ,/, uh/UH ,/, just/RB recently/RB put/VBD a/DT security/NN system/NN in/IN our/PRP$ house/NN and/CC so/RB now/RB ,/, uh/UH ,/, in/IN order/NN to/TO ,/, uh/UH ,/, to/TO accommodate/VB the/DT motion/NN detectors/NNS we/PRP have/VBP to/TO keep/VB her/PRP$ ,/, uh/UH ,/, uh/UH ,/, locked/VBN up/RP in/IN the/DT ,/, the/DT master/NN bedroom/NN during/IN the/DT day/NN and/CC then/RB she/PRP 's/HVS got/VBN the/DT ,/, the/DT bedroom/NN and/CC the/DT bathroom/NN to/IN ,/, for/IN free/JJ run/NN during/IN the/DT day/NN but/CC ,/,\">, <A.45: 'Uh-huh/UH ./.'>, <B.46: \"we/PRP 've/VBP always/RB got/VBN ,/, uh/UH ,/, got/VBN a/DT nose/NN and/CC tongue/NN pressed/VBN up/IN against/IN the/DT window/NN when/WRB we/PRP come/VBP walking/VBG up/IN to/IN the/DT front/JJ door/NN ./.\">, <A.47: './.'>, <B.48: \"She/PRP 's/BES definitely/RB ready/JJ to/TO get/VB out/RB and/CC run/VB around/RB ./.\">, <A.49: \"Well/UH my/PRP$ dog/NN 's/BES an/DT outdoor/JJ type/NN ,/, she/PRP does/VBZ not/RB like/VB to/TO be/VB indoors/RB ./.\">, <B.50: 'Really/UH ./.'>, <A.51: \"Uh/UH ,/, she/PRP 'd/MD rather/RB sle/VB ,/, sleep/VB outside/RB on/IN the/DT ,/, the/DT cold/JJ ground/NN at/IN night/NN ./.\">, <B.52: 'Oh/UH wow/UH ./.'>, <A.53: 'But/CC ,/, uh/UH ,/, I/PRP do/VBP make/VB her/PRP$ come/VB in/RB ./.'>, <B.54: 'Yeah/UH ./.'>, <A.55: \"And/CC I/PRP feed/VBP her/PRP$ indoors/RB ,/, that/DT 's/BES to/TO lure/VB her/PRP in/RB ,/, but/CC during/IN the/DT day/NN I/PRP have/VBP her/PRP on/IN a/DT ,/, uh/UH ,/, on/IN a/DT leash/NN ,/,\">, <B.56: 'Okay/UH ./.'>, <A.57: 'which/WDT is/VBZ ,/, uh/UH ,/, on/IN sort/RB of/RB a/DT run/NN ./. I/PRP have/VBP a/DT ,/, a/DT thirty/CD foot/NN cable/NN ,/,'>, <B.58: 'Okay/UH ./.'>, <A.59: 'running/VBG from/IN one/CD stake/NN to/IN another/DT ,/, and/CC then/RB attached/VBN to/IN that/DT is/VBZ a/DT ,/, uh/UH ,/, twelve/CD foot/NN leash/NN ,/,'>, <B.60: 'Okay/UH ./.'>, <A.61: 'so/RB she/PRP can/MD cover/VB quite/PDT an/DT area/NN ./.'>, <B.62: 'Most/JJS definitely/RB ./.'>, <A.63: \"And/CC ,/, uh/UH ,/, she/PRP 's/BES the/DT best/JJS ,/, uh/UH ,/, burglar/NN alarm/NN going/VBG ./.\">, <B.64: \"Yeah/UH ,/, yeah/UH ,/, yeah/UH that/DT 's/BES ,/, uh/UH ,/, definite/JJ security/NN involved/VBN in/IN ,/, uh/UH ,/, in/IN a/DT dog/NN like/IN that/DT ./.\">, <A.65: \"Oh/UH ,/, yeah/UH ,/, she/PRP ,/, uh/UH ,/, it/PRP 's/BES the/DT strangest/JJS thing/NN ,/, though/RB ,/, uh/UH ,/, children/NNS ,/, no/DT matter/NN how/WRB strange/JJ they/PRP are/VBP ,/, or/CC how/WRB new/JJ they/PRP might/MD be/VB can/MD walk/VB ,/, uh/UH ,/, right/RB up/IN to/IN her/PRP$ ,/,\">, <B.66: 'Uh-huh/UH ./.'>, <A.67: \"but/CC adults/NNS ,/, if/IN they/PRP 're/VBP strange/JJ to/IN her/PRP$ ,/, or/CC ,/, or/CC they/PRP look/VBP suspicious/JJ or/CC something/NN ,/, boy/UH she/PRP acts/VBZ like/IN she/PRP wants/VBZ to/TO chew/VB their/PRP$ leg/NN off/RP ./.\">, <B.68: 'Wow/UH ./.'>, <A.69: 'And/CC I/PRP have/VBP not/RB discovered/VBN yet/RB where/WRB the/DT ,/, the/DT line/NN is/VBZ between/IN children/NNS and/CC adults/NNS ./.'>, <B.70: \"Yeah/UH ,/, that/DT 's/BES interesting/JJ ./.\">, <A.71: 'But/CC ,/, uh/UH ,/, she/PRP is/VBZ a/DT great/JJ comfort/NN to/IN me/PRP ./.'>, <B.72: \"Yeah/UH ,/, I/PRP know/VBP our/PRP$ dog/NN has/VBZ had/VBN ,/, uh/UH ,/, some/DT different/JJ reactions/NNS ,/, she/PRP 's/HVS never/RB really/RB been/VBN around/IN children/NNS and/CC ,/, uh/UH ,/, if/IN ,/, if/IN the/DT child/NN is/VBZ ,/, is/VBZ pretty/RB straight/GW forward/^JJ ,/, um/UH ,/, she/PRP 's/BES fine/JJ ./. If/IN ,/, if/IN a/DT child/NN is/VBZ a/DT little/RB intimidated/JJ ,/, she/PRP 'll/MD jump/VB around/RB and/CC ,/, and/CC yip/VB and/CC bark/VB quite/PDT a/DT bit/NN ,/, and/CC if/IN the/DT child/NN gets/VBZ scared/JJ ,/, uh/UH ,/, she/PRP 's/BES still/RB trying/VBG to/TO play/VB ,/, but/CC she/PRP does/VBZ n't/RB completely/RB understand/VB what/WP 's/BES going/VBG on/RP and/CC we/PRP 've/VBP had/VBN a/DT little/JJ confusion/NN with/IN ,/, with/IN ,/, uh/UH ,/, with/IN younger/JJR kids/NNS ./.\">, <A.73: 'Uh-huh/UH ./.'>, <B.74: \"But/CC ,/, uh/UH ,/, you/PRP know/VBP ,/, that/DT 's/BES ,/, it/PRP 's/BES a/DT matter/NN of/IN exposure/NN really/RB ./. Um/UH ,/, we/PRP ,/, uh/UH ,/, took/VBD her/PRP$ home/NN to/IN ,/, uh/UH ,/, my/PRP$ family/NN 's/POS place/NN in/IN South/NNP Dakota/NNP ,/, and/CC she/PRP was/VBD the/DT one/NN that/WDT was/VBD intimidated/JJ then/RB ./. There/RB was/VBD about/RB seven/CD kids/NNS ranging/VBG from/IN about/RB ,/, uh/UH ,/, three/CD years/NNS to/IN ten/CD years/NNS running/VBG around/IN the/DT house/NN all/RB at/IN one/CD time/NN ,/,\">, <A.75: 'Oh/UH ,/, uh-huh/UH ./.'>, <B.76: \"you/PRP know/VBP come/VB to/TO visit/VB Grandpa/NNP and/CC Grandma/NNP and/CC ,/, the/DT dog/NN kind/RB of/RB ,/, kind/RB of/RB felt/VBD out/IN of/IN place/NN then/RB because/IN she/PRP was/VBD ,/, she/PRP was/VBD being/VBG fed/VBN ,/, and/CC everything/NN else/RB from/IN all/DT directions/NNS ./. She/PRP really/RB did/VBD n't/RB know/VB how/WRB to/TO handle/VB herself/PRP ./.\">, <A.77: \"You/PRP mean/VBP she/PRP did/VBD n't/RB appreciate/VB all/PDT that/DT attention/NN ./.\">, <B.78: \"She/PRP really/RB did/VBD ,/, she/PRP just/RB ,/, uh/UH ,/, she/PRP ,/, she/PRP was/VBD alm-/XX ,/, she/PRP was/VBD just/RB inundated/VBN with/IN ,/, with/IN all/PDT the/DT attention/NN ./. Uh/UH ,/, she/PRP ,/, she/PRP kind/RB of/RB ,/, she/PRP kind/RB of/RB sat/VBD and/CC MUMBLEx/XX it/PRP all/DT in/RB for/IN a/DT little/JJ while/IN and/CC then/RB she/PRP 'd/MD go/VB get/VB back/RB in/RB and/CC try/VB to/TO play/VB and/CC ,/, and/CC what/WP not/RB ,/, but/CC ,/, uh/UH ,/, it/PRP was/VBD ,/, it/PRP was/VBD just/RB such/PDT a/DT ,/, such/JJ a/DT new/JJ experience/NN for/IN her/PRP$ ./. She/PRP 's/HVS only/RB been/VBN around/RB one/CD and/CC ,/, and/CC sometimes/RB two/CD people/NNS at/IN the/DT most/RBS so/RB ,/, uh/UH ./.\">, <A.79: \"Uh-huh/UH ./. What/WP 's/BES her/PRP$ name/NN by/IN the/DT way/NN ?/.\">, <B.80: 'Uh/UH ,/, pardon/UH ?/.'>, <A.81: 'What/WP ,/, what/WP do/VBP you/PRP call/VB the/DT dog/NN ?/.'>, <B.82: \"Oh/UH ,/, it/PRP 's/BES ,/, uh/UH ,/, Mitzi/NNP ./.\">, <A.83: 'Mitzi/NNP ./.'>, <B.84: 'Yeah/UH ./.'>, <A.85: 'Mine/PRP is/VBZ Gin/NNP ./.'>, <B.86: 'Oh/UH ,/, okay/UH ./.'>, <A.87: 'As/IN in/IN ,/, uh/UH ,/, martini/NN ./.'>, <B.88: 'Yeah/UH ./.'>, <A.89: \"Actually/UH ,/, it/PRP 's/BES Gin/NNP two/CD ./.\">, <B.90: 'I/PRP ,/, I/PRP see/VBP ./.'>, <A.91: \"Because/IN ,/, uh/UH ,/, when/WRB I/PRP was/VBD a/DT teenager/NN ,/, in/IN high/JJ school/NN ,/, I/PRP had/VBD Gin/NNP one/CD ,/, but/CC then/RB when/WRB I/PRP went/VBD out/IN in/IN the/DT world/NN ,/, I/PRP could/MD n't/RB take/VB her/PRP with/IN me/PRP ./.\">, <B.92: 'Yeah/UH ,/, yeah/UH ,/, yeah/UH ,/, I/PRP had/VBD ,/, uh/UH ,/, a/DT similar/JJ ,/, uh/UH ,/, experience/NN ./. I/PRP ,/, I/PRP grew/VBD up/RP on/IN a/DT farm/NN so/RB I/PRP always/RB had/VBD ,/, uh/UH ,/, outdoor/JJ pets/NNS and/CC ,/,'>, <A.93: 'Uh-huh/UH ./.'>, <B.94: 'uh/UH ,/, the/DT dog/NN I/PRP had/VBD when/WRB I/PRP moved/VBD to/IN Dallas/NNP about/RB five/CD years/NNS ago/RB was/VBD a/DT ,/, uh/UH ,/, Springer/NNP Spaniel/NNP ,/, black/JJ Lab/NNP cross/NN ./. And/CC he/PRP was/VBD a/DT real/RB ,/, a/DT real/RB lovable/JJ type/NN ,/,'>, <A.95: 'Oh/UH yeah/UH ./.'>, <B.96: 'but/CC ,/, uh/UH ,/, definitely/RB not/RB an/DT apartment/NN type/NN animal/NN so/RB he/PRP ,/, uh/UH ,/, he/PRP had/VBD to/TO stay/VB home/NN ./.'>, <A.97: 'Oh/UH ,/, what/WP a/DT shame/NN ./.'>, <B.98: 'Yeah/UH ,/, yeah/UH ,/, it/PRP really/RB was/VBD ./. He/PRP ,/, uh/UH ,/, apparently/RB had/VBD a/DT tough/JJ time/NN with/IN it/PRP for/IN a/DT little/JJ while/IN and/CC then/RB he/PRP ,/, he/PRP got/VBD ,/, he/PRP came/VBD to/TO accept/VB the/DT fact/NN that/DT Mom/NNP and/CC Dad/NNP were/VBD his/PRP$ company/NN from/IN then/RB on/RB ,/, but/CC ,/, uh/UH ./.'>, <A.99: 'Uh-huh/UH ,/, oh/UH well/UH ./.'>, <B.100: 'Yeah/UH ./.'>, <A.101: \"Well/UH Randy/UH ,/, we/PRP 've/VBP just/RB about/IN used/VBN up/RP our/PRP$ time/NN here/RB ,/,\">, <B.102: 'Okay/UH ./.'>, <A.103: 'and/CC I/PRP must/MD say/VB it/PRP was/VBD interesting/JJ ./.'>, <B.104: 'Most/JJS definitely/RB ./.'>, <A.105: 'I/PRP enjoyed/VBD talking/VBG about/IN pets/NNS with/IN you/PRP ./.'>, <B.106: \"Well/UH that/DT 's/BES great/JJ ./.\">, <A.107: \"Maybe/RB we/PRP 'll/MD get/VB together/RB again/RB in/IN the/DT future/NN ./.\">, <B.108: 'That/DT sounds/VBZ real/RB good/JJ ./.'>, <A.109: 'Take/VB care/NN now/RB ./.'>, <B.110: 'You/PRP too/RB ./.'>, <A.111: 'Bye/UH ./.'>], [<A.1: 'Yes/UH ,/, um/UH ,/, I/PRP was/VBD wondering/VBG whether/IN you/PRP were/VBD in/IN favor/NN of/IN statehood/NN ,/, independence/NN ,/, or/CC the/DT status/NN quo/FW for/IN Puerto/NNP Rico/NNP ./.'>, <B.2: \"I/PRP was/VBD a/DT resident/NN ,/, although/IN very/JJ young/NN of/IN the/DT sta-/NN ,/, of/IN what/WP is/VBZ now/RB the/DT state/NN of/IN Alaska/NNP in/IN nineteen/CD fifty-nine/CD when/WRB Alaska/NNP stopped/VBD being/VBG a/DT territory/NN and/CC became/VBD a/DT state/NN ./. Uh/UH ,/, so/RB I/PRP guess/VBP I/PRP have/VBP a/DT left/VB over/IN positive/JJ feeling/NN about/IN the/DT question/NN even/RB though/IN I/PRP do/VBP n't/RB know/VB very/RB much/RB about/IN Puerto/NNP Rico/NNP ./. Uh/UH ,/, I/PRP know/VBP that/IN all/PDT the/DT things/NNS that/WDT happened/VBD ,/, relative/JJ to/IN that/DT territory/NN in/IN Alaska/NNP have/VBP been/VBN very/RB positive/JJ ,/, uh/UH ,/, and/CC I/PRP have/VBP a/DT suspicion/NN that/IN ,/, that/IN I/PRP believe/VBP that/IN a/DT ,/, the/DT statehood/NN is/VBZ a/DT good/JJ idea/NN whenever/WRB you/PRP have/VBP a/DT territory/NN the/DT size/NN of/IN Puerto/NNP Rico/NNP one/PRP ought/MD either/DT to/TO make/VB it/PRP a/DT full-fledged/JJ state/NN or/CC ,/, or/CC let/VB it/PRP go/VB ,/, one/CD or/CC the/DT other/NN ./. What/WP is/VBZ your/PRP$ situation/NN ?/.\">, <A.3: \"Well/UH ,/, I/PRP 'm/VBP ,/, I/PRP guess/VBP I/PRP do/VBP n't/RB have/VB su-/XX ,/, such/JJ close/JJ experience/NN with/IN ,/, with/IN ,/, um/UH ,/, an/DT area/NN becoming/VBG a/DT state/NN as/IN you/PRP do/VBP ./. Um/UH ,/, my/PRP$ concern/NN is/VBZ the/DT economy/NN because/IN as/IN I/PRP understand/VBP it/PRP Puerto/NNP Rico/NNP has/VBZ a/DT very/RB low/JJ sta-/XX ,/, uh/UH ,/, standard/NN of/IN living/VBG or/CC at/IN least/JJS ,/, um/UH ,/, annual/JJ average/JJ income/NN ./. Um/UH ,/, part/NN of/IN this/DT I/PRP suppose/VBP is/VBZ justified/JJ in/IN ,/, in/IN that/DT the/DT climate/NN they/PRP do/VBP n't/RB need/VB ,/, perhaps/RB ,/, the/DT heating/NN and/CC the/DT housing/NN that/IN some/DT of/IN the/DT more/RBR northern/JJ territories/NNS need/VBP ./. However/RB ,/, in/IN that/DT case/NN ,/, I/PRP guess/VBP I/PRP would/MD favor/VB status/NN quo/FW ./. I/PRP have/VBP been/VBN to/IN Puerto/NNP Rico/NNP and/CC ,/, and/CC found/VBD it/PRP very/RB ,/, very/RB interesting/JJ ./. I/PRP did/VBD Peace/NNP Corps/NNP training/NN there/RB ./.\">, <B.4: \"Oh/UH that/DT 's/BES very/RB interesting/JJ ./.\">, <A.5: 'And/CC so/RB ,/, um/UH ,/, you/PRP know/VBP ,/, I/PRP saw/VBD it/PRP ,/, also/RB as/IN a/DT young/JJ student/NN and/CC ,/, and/CC it/PRP was/VBD very/RB foreign/JJ in/IN a/DT certain/JJ sense/NN ,/, although/IN I/PRP had/VBD grown/VBN up/RP in/IN California/NNP and/CC so/RB the/DT Spanish/JJ was/VBD no/DT problem/NN ./. Um/UH ,/, it/PRP was/VBD ,/, it/PRP was/VBD ,/, you/PRP know/VBP ,/, very/RB lovely/JJ and/CC ,/, and/CC the/DT people/NNS seemed/VBD very/RB friendly/JJ and/CC ,/, and/CC nice/JJ ./. I/PRP have/VBP ,/, actually/UH ,/, I/PRP work/VBP with/IN ,/, uh/UH ,/, a/DT girl/NN from/IN Puerto/NNP Rico/NNP and/CC ,/, I/PRP guess/VBP I/PRP have/VBP never/RB thought/VBN to/TO ask/VB her/PRP$ what/WP she/PRP favors/VBZ ,/, um/UH ./.'>, <B.6: 'Your/PRP$ concern/NN on/IN the/DT economy/NN was/VBD one/CD in/IN terms/NNS of/IN ,/, if/IN it/PRP became/VBD a/DT state/NN would/MD that/DT put/VB even/RB more/JJR pressures/NNS on/IN ,/, on/IN Puerto/NNP Rico/NNP ,/, or/CC pressures/NNS on/IN the/DT U/NNP ./. S/NNP ./. in/IN terms/NNS of/IN aid/NN or/CC ,/,'>, <A.7: \"Well/UH probably/RB more/JJR on/IN ,/, in/IN terms/NNS of/IN the/DT U/NNP ./. S/NNP ./. Um/UH ,/, you/PRP know/VBP ,/, I/PRP 'm/VBP ,/, I/PRP 'm/VBP not/RB quite/RB sure/JJ how/WRB the/DT U/NNP ./. S/NNP ./. copes/VBZ with/IN this/DT sort/NN of/IN thing/NN ./. I/PRP 've/VBP live/VB abro-/RB ,/, abroad/RB most/RBS of/IN my/PRP$ life/NN ,/, so/RB ,/, I/PRP guess/VBP I/PRP ,/, I/PRP 've/VBP been/VBN very/RB cut/VB off/RP ./. I/PRP lived/VBD actually/UH in/IN Lebanon/NNP ,/, so/RB ,/, I/PRP was/VBD very/RB cut/VB off/RP from/IN the/DT ,/, the/DT press/NN as/IN I/PRP ,/, you/PRP know/VBP ,/, and/CC be/VB ,/, was/VBD becoming/VBG an/DT adult/NN ./. So/RB ,/, I/PRP do/VBP n't/RB know/VB how/WRB the/DT U/NNP ./. S/NNP ./. would/MD cope/VB with/IN ,/, uh/UH ,/, a/DT new/JJ state/NN that/WDT is/VBZ so/RB very/RB ,/, very/RB poor/JJ ./.\">, <B.8: 'Uh-huh/UH ./.'>, <A.9: \"Um/UH ,/, I/PRP guess/VBP ,/, you/PRP know/VBP ,/, we/PRP 'd/MD have/VB a/DT lot/NN of/IN aid/NN ,/, if/IN ,/, if/IN you/PRP consider/VBP the/DT inner/JJ cities/NNS of/IN like/UH New/NNP York/NNP and/CC ,/, and/CC how/WRB much/JJ aid/NN it/PRP needs/VBZ ./. I/PRP suppose/VBP the/DT whole/JJ country/NN ,/, or/CC the/DT whole/NN ,/, um/UH ,/, new/JJ state/NN would/MD require/VB such/JJ aid/NN ./.\">, <B.10: \"I/PRP guess/VBP ,/, um/UH ,/, we/PRP 're/VBP assuming/VBG that/IN ,/, that/IN Puerto/NNP Rico/NNP would/MD be/VB per/IN capita/NNS significantly/RB more/RBR poor/JJ perhaps/RB than/IN ,/, than/IN say/UH Mississippi/NNP or/CC ,/,\">, <A.11: 'I/PRP think/VBP so/RB ./.'>, <B.12: \"Okay/UH ,/, that/DT 's/BES something/NN that/WDT ,/, that/WDT I/PRP guess/VBP I/PRP have/VBP not/RB much/JJ of/IN an/DT image/NN of/IN other/JJ than/IN ,/, than/IN Puerto/NNP Rico/NNP as/IN a/DT tropical/JJ island/NN and/CC consequently/RB large/JJ numbers/NNS of/IN barefoot/JJ natives/NNS or/CC something/NN not/RB in/IN a/DT perjuritive/JJ sense/NN but/CC in/IN a/DT ,/, in/IN a/DT carefree/JJ sense/NN ,/, I/PRP guess/VBP ./. Although/IN I/PRP 'm/VBP aware/JJ of/IN the/DT political/JJ problems/NNS and/CC unrest/NN ,/, and/CC ,/, and/CC also/RB difficulties/NNS they/PRP have/VBP ./. Um/UH ,/, I/PRP do/VBP n't/RB know/VB whether/IN state/NN woul/MD ,/, statehood/NN would/MD improve/VB their/PRP$ economy/NN ./. I/PRP do/VBP n't/RB ,/, I/PRP do/VBP n't/RB know/VB that/IN the/DT ve-/XX ,/, the/DT act/NN of/IN being/VBG a/DT state/NN would/MD have/VB any/DT impact/NN on/IN ,/, on/IN them/PRP ./. Uh/UH ,/, I/PRP guess/VBP they/PRP would/MD have/VB the/DT ability/NN to/TO do/VB some/DT taxing/NN that/IN they/PRP do/VBP n't/RB have/VB now/RB ./. But/CC of/IN course/NN if/IN their/PRP$ economy/NN is/VBZ weak/JJ the-/EX ,/, there/EX is/VBZ not/RB much/JJ of/IN a/DT base/NN on/IN which/WDT to/TO tax/VB ./. I/PRP do/VBP n't/RB know/VB if/IN they/PRP suffer/VBP in/IN a/DT sense/NN of/IN income/NN loss/NN as/IN being/VBG ,/, since/IN they/PRP are/VBP n't/RB a/DT state/NN ,/, whether/IN there/EX are/VBP monies/NNS that/WDT escape/VBP them/PRP so/RB to/TO speak/VB because/IN they/PRP are/VBP n't/RB able/JJ to/TO tax/VB like/IN a/DT typical/JJ state/NN would/MD be/VB ./. How/WRB do/VBP you/PRP feel/VB ,/, though/RB ,/, about/IN ,/, well/UH I/PRP guess/VBP it/PRP 's/BES to/IN their/PRP$ advantage/NN to/TO be/VB a/DT territory/NN ,/, but/CC ,/, um/UH ,/, I/PRP wonder/VBP how/WRB have/VBP ,/, having/VBG been/VBN in/IN a/DT territory/NN but/CC only/RB as/IN a/DT young/JJ student/NN and/CC my/PRP$ parents/NNS were/VBD in/IN the/DT military/NN at/IN the/DT time/NN so/RB they/PRP did/VBD n't/RB have/VB ready/JJ negative/JJ feelings/NNS about/IN being/VBG in/IN ,/, in/IN Alaska/NNP at/IN the/DT time/NN since/IN they/PRP voted/VBD absentee/JJ ./. I/PRP ,/, I/PRP would/MD imagine/VB that/IN it/PRP must/MD be/VB a/DT little/JJ bit/NN of/IN a/DT se-/XX ,/, feeling/NN of/IN second/JJ class/NN citizenry/NN ,/, uh/UH ,/, to/TO be/VB in/IN a/DT territory/NN that/RB large/JJ and/CC not/RB being/VBG able/JJ to/TO vote/VB ./. The/DT District/NNP of/IN Columbia/NNP people/NNS ,/, for/IN instance/NN are/VBP quite/RB frustrated/JJ ,/, I/PRP think/VBP ,/, at/IN times/NNS in/IN their/PRP$ not/RB having/VBG a/DT Senate/NNP representative/NN ./.\">, <A.13: \"Uh-huh/UH ./. Yeah/UH ,/, those/DT ,/, are/VBP good/JJ points/NNS ,/, um/UH ,/, which/WDT obviously/RB I/PRP had/VBD never/RB thought/VBN about/RB ./. Uh/UH ,/, I/PRP do/VBP n't/RB know/VB ,/, uh/UH ,/, I/PRP suppose/VBP they/PRP also/RB ,/, not/RB being/VBG a/DT state/NN are/VBP probably/RB freer/JJR to/TO determine/VB their/PRP$ own/JJ ,/, um/UH ,/, ways/NNS of/IN life/NN than/IN they/PRP would/MD if/IN ,/, if/IN ,/, I/PRP 'm/VBP trying/VBG to/TO think/VB exactly/RB what/WP is/VBZ imposed/VBN if/IN they/PRP would/MD become/VB a/DT state/NN versus/IN a/DT territory/NN ./.\">, <B.14: 'Uh-huh/UH ./.'>, <A.15: 'Perhaps/RB compulsory/JJ education/NN ./.'>, <B.16: 'Uh/UH ,/, I/PRP see/VBP what/WP you/PRP mean/VBP ,/, so/IN that/IN ,/,'>, <A.17: 'Um/UH ,/, and/CC taxes/NNS ./.'>, <B.18: \"Yeah/UH ,/, taxes/NNS would/MD undoubtedly/RB be/VB occurring/VBG ./. Uh/UH ,/, there/EX ,/, I/PRP do/VBP n't/RB know/VB ,/, I/PRP do/VBP n't/RB even/RB understand/VB exactly/RB how/WRB taxes/NNS are/VBP handled/VBN in/IN a/DT territorial/JJ situation/NN ./. Um/UH ,/, frequently/RB the/DT laws/NNS in/IN a/DT territory/NN are/VBP ,/, are/VBP in/IN some/DT ways/NNS as/RB stringent/JJ as/IN they/PRP are/VBP in/IN other/JJ states/NNS ,/, I/PRP ,/, and/CC I/PRP 'm/VBP thinking/VBG in/IN terms/NNS of/IN education/NN ./. But/CC then/RB again/RB each/DT state/NN sets/VBZ its/PRP$ own/JJ and/CC I/PRP do/VBP n't/RB know/VB how/WRB a/DT territorial/JJ governor/NN takes/VBZ care/NN of/IN something/NN like/IN Puerto/NNP Rico/NNP ./. Um/UH ,/, it/PRP 's/BES interesting/JJ because/IN I/PRP have/VBP n't/RB thought/VBN of/IN them/PRP in/IN terms/NNS of/IN the/DT problems/NNS relative/JJ to/IN ,/, uh/UH ,/, economy/NN ,/, uh/UH ,/, I/PRP had/VBD thought/VBN of/IN it/PRP more/JJR in/IN terms/NNS of/IN political/JJ ,/, uh/UH ,/, realization/NN and/CC I/PRP guess/VBP I/PRP had/VBD automatically/RB made/VBN the/DT assumption/NN that/IN ,/, gee/UH ,/, anyone/NN would/MD rather/RB be/VB a/DT state/NN of/IN the/DT U/NNP ./. S/NNP ./. than/IN an/DT independent/JJ country/NN so/RB ,/,\">, <A.19: \"Uh-huh/UH ./. Well/UH ,/, well/UH actually/UH I/PRP thin/VBP ,/, one/CD ,/, one/CD thing/NN that/WDT I/PRP remember/VBP hearing/VBG in/IN the/DT news/NN the/DT past/JJ couple/NN weeks/NNS that/WDT might/MD be/VB significant/JJ is/VBZ that/IN they/PRP 've/VBP recently/RB voted/VBD that/IN Spanish/JJ is/VBZ the/DT official/JJ language/NN ,/, which/WDT I/PRP always/RB assumed/VBD it/PRP was/VBD anyway/RB ./. So/RB ,/, if/IN they/PRP 've/VBP just/RB taken/VBN such/JJ action/NN ,/, it/PRP would/MD seem/VB to/TO indicate/VB to/IN me/PRP either/DT they/PRP 're/VBP doing/VBG it/PRP because/IN they/PRP 're/VBP afraid/JJ they/PRP might/MD become/VB a/DT state/NN and/CC want/VBP to/TO declare/VB this/DT before/IN they/PRP become/VBP a/DT state/NN ,/, or/CC maybe/RB because/IN they/PRP do/VBP n't/RB want/VB to/TO become/VB a/DT state/NN for/IN fear/NN of/IN losing/VBG the/DT Spanish/JJ ,/, or/CC Hispanic/JJ heritage/NN ./.\">, <B.20: \"Uh-huh/UH ,/, well/UH that/DT 's/BES interesting/JJ too/RB ./. Well/UH it/PRP sounds/VBZ as/IN though/IN you/PRP are/VBP in/IN favor/NN of/IN ,/, uh/UH ,/, status/NN quo/FW ./.\">, <A.21: 'I/PRP ,/, I/PRP believe/VBP I/PRP would/MD probably/RB tend/VB towards/IN that/DT ./.'>, <B.22: \"And/CC I/PRP think/VBP ,/, uh/UH ,/, having/VBG listened/VBN to/IN you/PRP relative/JJ to/IN the/DT economy/NN thing/NN ,/, I/PRP think/VBP if/IN I/PRP were/VBD being/VBG forced/VBN to/TO make/VB a/DT decision/NN I/PRP would/MD plead/VB ignorance/NN and/CC wait/VB to/TO do/VB more/JJR research/NN before/IN picking/VBG one/CD of/IN these/DT ./. So/RB I/PRP 'm/VBP ul-/RB ,/, I/PRP guess/VBP I/PRP 'm/VBP ultimately/RB in/IN favor/NN of/IN status/NN quo/FW also/RB at/IN this/DT point/NN ,/,\">, <A.23: \"Well/UH that/DT 's/BES interesting/JJ ./.\">, <B.24: 'leaning/VBG towards/IN the/DT statehood/NN ./.'>, <A.25: 'Right/UH ./. What/WP about/IN if/IN ,/, if/IN ,/, um/UH ,/, they/PRP demanded/VBD to/TO have/VB Spanish/JJ as/IN the/DT official/JJ language/NN as/IN a/DT condition/NN for/IN statehood/NN ./.'>, <B.26: \"I/PRP think/VBP I/PRP would/MD be/VB troubled/JJ ,/, I/PRP suspect/VBP ,/, I/PRP believe/VBP that/IN any/DT of/IN our/PRP$ states/NNS ,/, and/CC I/PRP ,/, constitutionally/RB I/PRP do/VBP n't/RB think/VB there/EX are/VBP any/DT prescriptions/NNS abo-/XX ,/, against/IN that/DT decision/NN even/RB if/IN Louisiana/NNP chose/VBD to/TO go/VB with/IN Creole/NNP or/CC something/NN ./. Um/UH ,/, I/PRP do/VBP n't/RB think/VB there/EX is/VBZ any/DT prohibitions/NNS ./. I/PRP would/MD be/VB bothered/JJ by/IN that/DT ./. I/PRP 'm/VBP ,/, I/PRP 'm/VBP bothered/JJ by/IN any/DT tendency/NN to/TO resist/VB what/WP I/PRP think/VBP was/VBD one/CD of/IN America/NNP 's/POS strengths/NNS and/CC that/DT 's/BES the/DT ,/, the/DT ,/, the/DT melting/VBG pot/NN ./.\">, <A.27: 'Uh-huh/UH ./.'>, <B.28: \"Uh/UH ,/, I/PRP ,/, I/PRP am/VBP particularly/RB fond/VB of/IN a/DT number/NN of/IN ethnic/JJ cuisines/NNS ,/, but/CC I/PRP 'm/VBP troubled/JJ by/IN too/RB close/JJ a/DT clinging/VBG to/IN ,/, to/IN the/DT past/NN and/CC I/PRP 'm/VBP also/RB realizing/VBG that/IN ,/, that/IN a/DT common/JJ language/NN ,/, I/PRP think/VBP ,/, is/VBZ the/DT ultimate/JJ bond/NN of/IN a/DT country/NN ./.\">, <A.29: 'Uh-huh/UH ./.'>, <B.30: \"And/CC Canada/NNP ,/, I/PRP guess/VBP ,/, comes/VBZ to/IN my/PRP$ mind/NN as/IN they/PRP 're/VBP going/VBG through/IN the/DT throes/NNS up/IN there/RB relative/JJ to/IN French/NNP Quebec/NNP and/CC whatever/WDT ./. Uh/UH ,/, that/IN I/PRP ,/, I/PRP think/VBP that/DT would/MD be/VB a/DT negative/JJ step/NN to/TO make/VB and/CC I/PRP think/VBP that/DT that/DT they/PRP would/MD need/VB to/TO reassess/VB that/DT ./. Um/UH ,/, I/PRP 'm/VBP troubled/JJ even/RB in/IN by/IN bilingualism/NN ,/, uh/UH ,/, in/GW so/GW far/^IN that/IN it/PRP intr-/XX ,/, it/PRP gets/VBZ in/IN the/DT way/NN of/IN ,/, of/IN ,/, of/IN the/DT melting/VBG pot/NN aspect/NN ./.\">, <A.31: \"Uh-huh/UH ,/, uh-huh/UH ./. That/DT 's/BES interesting/JJ because/IN although/IN I/PRP tend/VBP to/TO be/VB bilingual/JJ ,/, Spanish/JJ based/VBN on/IN experience/NN ,/, um/UH ,/, I/PRP was/VBD in/IN bilingual/JJ education/NN in/IN California/NNP and/CC I/PRP did/VBD n't/RB have/VB any/DT problem/NN with/IN my/PRP$ students/NNS but/CC I/PRP noticed/VBD my/PRP$ ,/, my/PRP$ brother/NN ,/, who/WP was/VBD quite/RB a/DT bit/NN younger/JJR ,/, was/VBD learning/VBG Spanish/NNP in/IN elementary/JJ school/NN and/CC he/PRP ca/MD n't/RB speak/VB a/DT word/NN ./.\">, <B.32: 'Um/UH ./.'>, <A.33: \"And/CC so/RB ,/, obviously/RB it/PRP did/VBD n't/RB work/VB in/IN California/NNP ./.\">, <B.34: \"Yeah/UH ,/, but/CC I/PRP 'm/VBP rather/RB in/IN favor/NN of/IN people/NNS being/VBG bilingual/JJ ,/,\">, <A.35: 'Right/UH ./.'>, <B.36: \"and/CC I/PRP 'd/MD be/VB quite/RB happy/JJ to/TO see/VB a/DT national/JJ law/NN in/IN which/WDT every/DT student/NN was/VBD required/VBN to/TO learn/VB English/NNP and/CC a/DT second/JJ language/NN ./.\">, <A.37: 'Right/UH ./.'>, <B.38: \"But/CC I/PRP 'm/VBP ,/, I/PRP 'm/VBP disturbed/JJ by/IN a/DT country/NN that/WDT attempts/VBZ to/TO be/VB functionally/RB bilingual/JJ at/IN the/DT official/JJ level/NN ./.\">, <A.39: 'Oh/UH ,/, I/PRP see/VBP ./.'>, <B.40: \"Um/UH ,/, I/PRP 'm/VBP ,/, I/PRP 'm/VBP concerned/JJ about/IN whether/IN or/CC not/RB that/DT causes/VBZ fractiousness/NN I/PRP guess/VBP ./.\">, <A.41: 'Uh-huh/UH ,/, uh-huh/UH ./.'>, <B.42: 'Well/UH I/PRP think/VBP we/PRP have/VBP gone/VBN to/IN time/NN and/CC I/PRP appreciate/VBP your/PRP$ having/VBG called/VBN ./.'>, <A.43: 'Okay/UH ,/, well/UH thank/VB you/PRP very/RB much/RB ./.'>, <B.44: 'Good-bye/UH ./.'>, <A.45: 'Bye/UH ,/, bye/UH ./.'>], ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uh , do you have a pet Randy ?'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "s = str(turn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " 'append',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'extend',\n",
       " 'id',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'pop',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'sort',\n",
       " 'speaker',\n",
       " 'unicode_repr']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(discourse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_reports_meta = {'title':'Problem Report Corpus',\n",
    "            'description': str(\n",
    "                           \"3.3% of the COMTRANS data, distributed with permission.\\n\\n\"\n",
    "                           )+format_description,\n",
    "            'authors': 'Reinhard Rapp',\n",
    "            'source': 'http://www.fask.uni-mainz.de/user/rapp/comtrans/',\n",
    "            'cite': '',\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
