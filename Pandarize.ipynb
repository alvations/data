{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from lazyme import per_section, deduplicate, find_files\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_nltk_data = nltk.data.path[0]\n",
    "\n",
    "new_nltk_data = \"packages/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC corpus.\n",
    "directory = new_nltk_data+'/corpora/abc/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with io.open(old_nltk_data+'/corpora/abc/rural.txt') as fin:\n",
    "    rural_texts = [line.strip() for line in fin if line.strip()]\n",
    "with io.open(old_nltk_data+'/corpora/abc/science.txt', encoding='latin_1') as fin:\n",
    "    science_texts = [line.strip().encode('utf8').decode('utf8') for line in fin if \n",
    "                    line.strip().encode('utf8').decode('utf8')]\n",
    "\n",
    "rural_df = pd.DataFrame({'text':rural_texts})\n",
    "rural_df['subcorpora'] = 'Rural News'\n",
    "\n",
    "science_df = pd.DataFrame({'text':science_texts})\n",
    "science_df['subcorpora'] = 'Science News'\n",
    "\n",
    "df_abc = pd.concat([rural_df, science_df])\n",
    "df_abc.to_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', index=False)\n",
    "df_abc = pd.read_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'subcorpora':str})\n",
    "\n",
    "abc_meta = {'title':'Australian Broadcasting Commission 2006',\n",
    "            'source': 'http://www.abc.net.au/',\n",
    "            'subcorpora': {'Rural News': {'source': 'http://www.abc.net.au/rural/news/'},\n",
    "                           'Science News': {'source': 'http://www.abc.net.au/science/news/'}\n",
    "                          },\n",
    "             'xml': {'id':'abc', 'name':\"Australian Broadcasting Commission 2006\",\n",
    "                     'webpage':\"http://www.abc.net.au/\", 'author':\"Australian Broadcasting Commission\",\n",
    "                      'unzip':\"1\"}\n",
    "           }\n",
    "\n",
    "\n",
    "abc_xml = ET.Element(\"package\", id=\"abc\", name=\"Australian Broadcasting Commission 2006\",\n",
    "                  webpage=\"http://www.abc.net.au/\", author=\"Australian Broadcasting Commission\",\n",
    "                  unzip=\"1\")\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(new_nltk_data+'/corpora/abc/abc.xml')\n",
    "\n",
    "with open(new_nltk_data+'/corpora/abc/abc-meta.json', 'w') as fout:\n",
    "    json.dump(abc_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subcorpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PM denies knowledge of AWB kickbacks</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Prime Minister has denied he knew AWB was ...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Letters from John Howard and Deputy Prime Mini...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In one of the letters Mr Howard asks AWB manag...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Opposition's Gavan O'Connor says the lette...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  subcorpora\n",
       "0               PM denies knowledge of AWB kickbacks  Rural News\n",
       "1  The Prime Minister has denied he knew AWB was ...  Rural News\n",
       "2  Letters from John Howard and Deputy Prime Mini...  Rural News\n",
       "3  In one of the letters Mr Howard asks AWB manag...  Rural News\n",
       "4  The Opposition's Gavan O'Connor says the lette...  Rural News"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown\n",
    "\n",
    "directory = new_nltk_data+'/corpora/brown/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "    \n",
    "with open(old_nltk_data+'/corpora/brown/cats.txt') as fin:\n",
    "     categories = {line.strip().split(' ')[0]:line.strip().split(' ')[1] \n",
    "                   for line in fin}\n",
    "        \n",
    "brown_dir = old_nltk_data+'/corpora/brown/'\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(brown_dir):\n",
    "    if filename in ['CONTENTS', 'cats.txt', 'README']:\n",
    "        continue\n",
    "    cat = categories[filename]\n",
    "    with open(brown_dir+filename) as fin:\n",
    "        i = -1\n",
    "        for paragraph in fin.read().split('\\n\\n'):\n",
    "            if not paragraph.strip():\n",
    "                continue\n",
    "            i += 1\n",
    "            j = -1\n",
    "            for sent in paragraph.split('\\n'):\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                j += 1\n",
    "                raw = sent.strip()\n",
    "                text, pos = zip(*[word.split('/') for word in raw.split()])\n",
    "                rows.append({'filename': filename, \n",
    "                              'para_id': i, \n",
    "                              'sent_id': j, \n",
    "                              'raw_text': raw, \n",
    "                              'tokenized_text': ' '.join(text), \n",
    "                              'tokenized_pos': ' '.join(pos), \n",
    "                              'label': cat})\n",
    "                \n",
    "                \n",
    "df_brown = pd.DataFrame(rows)[['filename', 'para_id', 'sent_id', \n",
    "                              'raw_text', 'tokenized_text', 'tokenized_pos', 'label']]\n",
    "df_brown.to_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', index=False)\n",
    "\n",
    "df_brown = pd.read_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', \n",
    "                     dtype={'filename':str, 'para_id':int, 'sent_id':int,\n",
    "                             'raw_text':str, 'tokenized_text':str, 'tokenized_pos':str,\n",
    "                           'label':str})\n",
    "\n",
    "df_brown_cats = df_brown[['filename', 'label']].drop_duplicates().sort_values('filename')\n",
    "df_brown_cats.to_csv(new_nltk_data+'/corpora/brown/cats.tsv', sep='\\t', index=False)\n",
    "\n",
    "brown_readme = \"\"\"BROWN CORPUS\n",
    "\n",
    "A Standard Corpus of Present-Day Edited American\n",
    "English, for use with Digital Computers.\n",
    "\n",
    "by W. N. Francis and H. Kucera (1964)\n",
    "Department of Linguistics, Brown University\n",
    "Providence, Rhode Island, USA\n",
    "\n",
    "Revised 1971, Revised and Amplified 1979\n",
    "\n",
    "http://www.hit.uib.no/icame/brown/bcm.html\n",
    "\n",
    "Distributed with the permission of the copyright holder,\n",
    "redistribution permitted.\"\"\"\n",
    "\n",
    "brown_meta = {'title':'Brown Corpus',\n",
    "              'description': str('A Standard Corpus of Present-Day Edited American English, '\n",
    "                               'for use with Digital Computers.'),\n",
    "              'authors': 'W. N. Francis and H. Kucera (1964)',\n",
    "              'url': 'http://www.hit.uib.no/icame/brown/bcm.html',\n",
    "              'readme': brown_readme}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/brown/brown-meta.json', 'w') as fout:\n",
    "    json.dump(brown_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore/rb ,/, as/cs an/at encouragement/n...</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "      <td>rb , cs at nn in nn nn , pps rb bez jj to vb c...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The/at Unitarian/jj clergy/nns were/bed an/at ...</td>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "      <td>at jj nns bed at jj nn in vbn nns -- cs at nn ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Ezra/np Stiles/np Gannett/np ,/, an/at honorab...</td>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "      <td>np np np , at jj nn in at nn , vbd ppl rb in a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Even/rb so/rb ,/, Gannett/np judiciously/rb ar...</td>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "      <td>rb rb , np rb vbd , at nn-tl md rb vb cs np ``...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>We/ppss today/nr are/ber not/* entitled/vbn to...</td>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "      <td>ppss nr ber * vbn to vb jj nns wps vbd np to b...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  para_id  sent_id  \\\n",
       "0     cd05        0        0   \n",
       "1     cd05        0        1   \n",
       "2     cd05        0        2   \n",
       "3     cd05        0        3   \n",
       "4     cd05        0        4   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  Furthermore/rb ,/, as/cs an/at encouragement/n...   \n",
       "1  The/at Unitarian/jj clergy/nns were/bed an/at ...   \n",
       "2  Ezra/np Stiles/np Gannett/np ,/, an/at honorab...   \n",
       "3  Even/rb so/rb ,/, Gannett/np judiciously/rb ar...   \n",
       "4  We/ppss today/nr are/ber not/* entitled/vbn to...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  Furthermore , as an encouragement to revisioni...   \n",
       "1  The Unitarian clergy were an exclusive club of...   \n",
       "2  Ezra Stiles Gannett , an honorable representat...   \n",
       "3  Even so , Gannett judiciously argued , the Ass...   \n",
       "4  We today are not entitled to excoriate honest ...   \n",
       "\n",
       "                                       tokenized_pos     label  \n",
       "0  rb , cs at nn in nn nn , pps rb bez jj to vb c...  religion  \n",
       "1  at jj nns bed at jj nn in vbn nns -- cs at nn ...  religion  \n",
       "2  np np np , at jj nn in at nn , vbd ppl rb in a...  religion  \n",
       "3  rb rb , np rb vbd , at nn-tl md rb vb cs np ``...  religion  \n",
       "4  ppss nr ber * vbn to vb jj nns wps vbd np to b...  religion  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazetteers\n",
    "\n",
    "directory = new_nltk_data+'/corpora/gazetteers/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(old_nltk_data+'/corpora/gazetteers/'):\n",
    "    if filename in ['LICENSE.txt']:\n",
    "        continue\n",
    "    label = gazetteers_filename2labels[filename]\n",
    "    with io.open(old_nltk_data+'/corpora/gazetteers/'+filename, encoding='ISO-8859-2') as fin:\n",
    "        for line in fin:\n",
    "            if line.strip():\n",
    "                text = line.strip()\n",
    "                if text == 'QuerĂŠtaro':\n",
    "                    text = 'Querétaro'\n",
    "                rows.append({'text':text, 'label':label})\n",
    "\n",
    "df_gazetteers = pd.DataFrame(rows)[['text', 'label']]\n",
    "\n",
    "#alpabet = list('abcdefghijklmnopqrstuvwxyz. ()-,') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "#alpabet += [\"'\"]\n",
    "#[word for word in df_gazetteers['text'] if any(ch for ch in word if ch not in alpabet)]\n",
    "\n",
    "df_gazetteers.to_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', index=False)\n",
    "df_gazetteers = pd.read_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'label':str})\n",
    "\n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "gazetteers_meta = {'title':'Geolocation Gazeteers',\n",
    "                    'subcorpora': {'Mexico States': {'original_file': 'mexstates.txt'},\n",
    "                                   'Canada Provinces': {'original_file': 'caprovinces.txt'},\n",
    "                                   'US State Abbreviations': {'original_file': 'usstates.txt'},\n",
    "                                   'US States': {'original_file': 'usstateabbrev.txt'},\n",
    "                                   'US Cities': {'original_file':'uscities.txt',\n",
    "                                                 'source': 'http://en.wikipedia.org/wiki/List_of_cities_in_the_United_States_with_over_100%2C000_people',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries': {'original_file':'countries.txt',\n",
    "                                                 'source':'http://en.wikipedia.org/wiki/List_of_countries',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries ISO codes': {'original_file': 'isocountries.txt',\n",
    "                                                          'source': 'http://www.guavastudios.com/country-list.htm'\n",
    "                                                          },\n",
    "                                   'Nationalities': {'original_file': 'nationalities.txt',\n",
    "                                                    'source': 'http://www.guavastudios.com/nationalities-list.htm'\n",
    "                                                    },\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/gazetteers/gazetteers-meta.json', 'w') as fout:\n",
    "    json.dump(gazetteers_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "directory = new_nltk_data+'/corpora/words/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en') as fin:\n",
    "    for line in fin:\n",
    "        en_words.append(line.strip())\n",
    "\n",
    "basic_en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en-basic') as fin:\n",
    "    for line in fin:\n",
    "        basic_en_words.append(line.strip())\n",
    "        \n",
    "words_meta = {'title':'Word Lists',\n",
    "              'subcorpora': {'Unix Words':{'source':'http://en.wikipedia.org/wiki/Words_(Unix)'},\n",
    "                           'Ogden Basic English': {'title': 'The ABC of Basic English',\n",
    "                                                   'author':'C.K. Ogden (1932)'}\n",
    "                          }\n",
    "            }\n",
    "\n",
    "unix_words = pd.DataFrame({'text':en_words})\n",
    "ogden_words = pd.DataFrame({'text':basic_en_words})\n",
    "\n",
    "unix_words.to_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', index=False)\n",
    "ogden_words.to_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', index=False)\n",
    "\n",
    "unix_words = pd.read_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', dtype={'text':str})\n",
    "ogden_words = pd.read_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', dtype={'text':str})\n",
    "\n",
    "with open(new_nltk_data+'/corpora/words/words-meta.json', 'w') as fout:\n",
    "    json.dump(words_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = new_nltk_data+'/corpora/movie_reviews/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/pos/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "    \n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/pos/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'pos'\n",
    "                            })\n",
    "                \n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/neg/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "\n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/neg/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'neg'\n",
    "                            })\n",
    "                \n",
    "df_movie_reivews = pd.DataFrame(rows)[['fold_id', 'cv_tag', 'html_id', 'sent_id', 'text', 'tag']]\n",
    "\n",
    "df_movie_reivews.to_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', index=False)\n",
    "df_movie_reivews = pd.read_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', \n",
    "                     dtype={'fold_id':int, 'cv_tag':str, 'html_id':str, 'sent_id':int,\n",
    "                            'text':str, 'tag':str})\n",
    "\n",
    "\n",
    "mr_bibtext = \"\"\"@InProceedings{Pang+Lee:04a,\n",
    "  author =       {Bo Pang and Lillian Lee},\n",
    "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
    "  booktitle =    \"Proceedings of the ACL\",\n",
    "  year =         2004\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "mr_meta = {'title': 'Sentiment Polarity Dataset Version 2.0',\n",
    "           'aka': 'Moview Review Data',\n",
    "           'source': 'http://www.cs.cornell.edu/people/pabo/movie-review-data/',\n",
    "           'authors': 'Bo Pang and Lillian Lee',\n",
    "           'license': 'Distributed with NLTK with permission from the authors.',\n",
    "            'bibtex':mr_bibtext}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/movie_reviews/movie_reviews-meta.json', 'w') as fout:\n",
    "    json.dump(mr_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/webtext/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/webtext/')):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    subcorp = filename.split('.')[0]\n",
    "    with open(old_nltk_data+'/corpora/webtext/' + filename, encoding='latin-1') as fin:\n",
    "        for line in fin:\n",
    "            rows.append({'text': line.strip(), 'domain': subcorp})\n",
    "df_webtext = pd.DataFrame(rows)[['text', 'domain']]\n",
    "\n",
    "\n",
    "df_webtext.to_csv(new_nltk_data +'/corpora/webtext/webtext.tsv', sep='\\t', index=False)\n",
    "df_webtext = pd.read_csv(new_nltk_data + '/corpora/webtext/webtext.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'domain':str})\n",
    "\n",
    "\n",
    "\n",
    "webtext_meta = {'title':'Web Text Corpus',\n",
    "                   'description': str(\"This is a collection of diverse, contemporary text genres, \"\n",
    "                                      \"collected by scraping publicly accessible archives of web postings. \"\n",
    "                                      \"This data is disseminated in preference to publishing URLs for \"\n",
    "                                       \"individuals to download and clean up (the usual model for web corpora).\"),\n",
    "                   \n",
    "                    'subcorpora': {'firefox': {'original_file': 'firefox.txt', \n",
    "                                               'description': 'Firefox support forum'},\n",
    "                                   'overheard': {'original_file': 'overheard.txt', \n",
    "                                               'description': 'Overheard in New York (partly censored)', \n",
    "                                                'source': 'http://www.overheardinnewyork.com/', \n",
    "                                                'year': '2006'},\n",
    "                                   'pirate': {'original_file': 'pirate.txt', \n",
    "                                               'description': \"Movie script from Pirates of the Caribbean: Dead Man's Chest\",\n",
    "                                                'source': 'http://www.overheardinnewyork.com/', \n",
    "                                                'year': '2006'},\n",
    "                                   'grail': {'original_file': 'grail.txt', \n",
    "                                               'description': 'Movie script from Monty Python and the Holy Grail',\n",
    "                                                'source': 'http://www.textfiles.com/media/SCRIPTS/grail', \n",
    "                                                'year': '2006'},\n",
    "                                   'singles': {'original_file': 'singles.txt', \n",
    "                                               'description': 'Singles ads',\n",
    "                                                'source': 'http://search.classifieds.news.com.au/',},\n",
    "                                   'wine': {'original_file': 'wine.txt', \n",
    "                                               'description': 'Fine Wine Diary',\n",
    "                                                'source': 'http://www.finewinediary.com/', \n",
    "                                                'year': '2005-6'},\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/webtext/webtext-meta.json', 'w') as fout:\n",
    "    json.dump(webtext_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/names/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "\n",
    "rows =[]\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/names/')):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    label = filename.split('.')[0]\n",
    "    with open(old_nltk_data+'/corpora/names/' + filename) as fin:\n",
    "        for line in fin:\n",
    "            rows.append({'text': line.strip(), 'gender': label})\n",
    "        \n",
    "df_names = pd.DataFrame(rows)[['text', 'gender']]\n",
    "df_names.to_csv(new_nltk_data+'/corpora/names/names.tsv', sep='\\t', index=False)\n",
    "df_names = pd.read_csv(new_nltk_data+'/corpora/names/names.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'gender':str})\n",
    "            \n",
    "names_readme = \"\"\"Names Corpus, Version 1.3 (1994-03-29)\n",
    "Copyright (C) 1991 Mark Kantrowitz\n",
    "Additions by Bill Ross\n",
    "\n",
    "This corpus contains 5001 female names and 2943 male names, sorted\n",
    "alphabetically, one per line.\n",
    "\n",
    "You may use the lists of names for any purpose, so long as credit is\n",
    "given in any published work. You may also redistribute the list if you\n",
    "provide the recipients with a copy of this README file. The lists are\n",
    "not in the public domain (I retain the copyright on the lists) but are\n",
    "freely redistributable.  If you have any additions to the lists of\n",
    "names, I would appreciate receiving them.\n",
    "\n",
    "Mark Kantrowitz <mkant+@cs.cmu.edu>\n",
    "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\"\"\"\n",
    "    \n",
    "names_meta = {'title': 'Names Corpus',\n",
    "              'version': '1.3 (1994-03-29)',\n",
    "              'description': 'This corpus contains 5001 female names and 2943 male names',\n",
    "              'authors': 'Mark Kantrowitz, Bill Ross',\n",
    "              'license': 'Copyright (C) 1991 Mark Kantrowitz',\n",
    "              'source': 'http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/',\n",
    "              'readme': names_readme}\n",
    "\n",
    "\n",
    "with open(new_nltk_data+'/corpora/names/names-meta.json', 'w') as fout:\n",
    "    json.dump(names_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the Union\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "all_sotu = {}\n",
    "\n",
    "text_url = 'http://stateoftheunion.onetwothree.net/texts/'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "for li in BeautifulSoup(requests.get(text_url + 'index.html').content).find_all('li'):\n",
    "    if not li.find('a')['href']:\n",
    "        continue\n",
    "\n",
    "    sotu = text_url + li.find('a')['href']\n",
    "    if sotu.split('/')[-1].split('.')[0].isdigit():\n",
    "        year = li.find('a').text.split(', ')[-1]\n",
    "        if year in all_sotu:\n",
    "            continue\n",
    "        else:\n",
    "            soup = BeautifulSoup(requests.get(sotu, headers=headers).content)\n",
    "            name = soup.find('h2').text\n",
    "            date = soup.find('h3').text\n",
    "            year = date.split(', ')[1]\n",
    "            print(year, end=', ')\n",
    "            texts = [str(p) for p in soup.find_all('p')]\n",
    "            all_sotu[year] = {'year':year, 'date':date, 'name':name, 'texts':'\\n\\n'.join(texts).strip()}\n",
    "            \n",
    "for year in all_sotu:\n",
    "    lastname = all_sotu[year]['name'].split()[-1]\n",
    "    with open(f'sotu/{lastname}-{year}', 'w') as fout:\n",
    "        print(all_sotu[year]['texts'].strip(), file=fout)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/state_union/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "df_sotu = pd.read_csv('stateunion.tsv', sep='\\t', \n",
    "                     dtype={'texts':str, 'date':str, 'name':str},\n",
    "                     index_col=0)\n",
    "\n",
    "df_sotu.T['texts'] = ['\\n\\n'.join([deduplicate(' '.join(para.split('\\n')), ' ') \n",
    "                       for para in re.sub('<[^<]+?>', '', raw).strip().split('\\n\\n')])\n",
    "                      for raw in df_sotu.T.texts]\n",
    "\n",
    "sotu_meta = {'title': 'State of the Union: Addrresses', \n",
    "             'author': 'Brad Borevitz', \n",
    "             'source': 'http://stateoftheunion.onetwothree.net/texts/',\n",
    "             'year': '1790-2018'\n",
    "            }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/state_union/state_union-meta.json', 'w') as fout:\n",
    "    json.dump(sotu_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    \n",
    "df_sotu.to_csv(new_nltk_data+'/corpora/state_union/state_union.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "      <th>1798</th>\n",
       "      <th>1799</th>\n",
       "      <th>...</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>January 8, 1790</td>\n",
       "      <td>October 25, 1791</td>\n",
       "      <td>November 6, 1792</td>\n",
       "      <td>December 3, 1793</td>\n",
       "      <td>November 19, 1794</td>\n",
       "      <td>December 8, 1795</td>\n",
       "      <td>December 7, 1796</td>\n",
       "      <td>November 22, 1797</td>\n",
       "      <td>December 8, 1798</td>\n",
       "      <td>December 3, 1799</td>\n",
       "      <td>...</td>\n",
       "      <td>February 24, 2009</td>\n",
       "      <td>January 27, 2010</td>\n",
       "      <td>January 25, 2011</td>\n",
       "      <td>January 24, 2012</td>\n",
       "      <td>February 12, 2013</td>\n",
       "      <td>January 28, 2014</td>\n",
       "      <td>January 20, 2015</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>February 28, 2017</td>\n",
       "      <td>January 30, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>...</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "      <td>Madame Speaker, Vice President Biden, Members ...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1790  \\\n",
       "date                                     January 8, 1790   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1791  \\\n",
       "date                                    October 25, 1791   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1792  \\\n",
       "date                                    November 6, 1792   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1793  \\\n",
       "date                                    December 3, 1793   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1794  \\\n",
       "date                                   November 19, 1794   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1795  \\\n",
       "date                                    December 8, 1795   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1796  \\\n",
       "date                                    December 7, 1796   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1797  \\\n",
       "date                                   November 22, 1797   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1798  \\\n",
       "date                                    December 8, 1798   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1799  \\\n",
       "date                                    December 3, 1799   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                             ...                          \\\n",
       "date                         ...                           \n",
       "name                         ...                           \n",
       "texts                        ...                           \n",
       "\n",
       "                                                    2009  \\\n",
       "date                                   February 24, 2009   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Mr. Vice President, Members of...   \n",
       "\n",
       "                                                    2010  \\\n",
       "date                                    January 27, 2010   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Vice President Biden, Members ...   \n",
       "\n",
       "                                                    2011  \\\n",
       "date                                    January 25, 2011   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2012  \\\n",
       "date                                    January 24, 2012   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2013  \\\n",
       "date                                   February 12, 2013   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2014  \\\n",
       "date                                    January 28, 2014   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2015  \\\n",
       "date                                    January 20, 2015   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2016  \\\n",
       "date                                    January 12, 2016   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2017  \\\n",
       "date                                   February 28, 2017   \n",
       "name                                     Donald J. Trump   \n",
       "texts  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "\n",
       "                                                    2018  \n",
       "date                                    January 30, 2018  \n",
       "name                                     Donald J. Trump  \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sotu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "      <th>1798</th>\n",
       "      <th>1799</th>\n",
       "      <th>...</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>January 8, 1790</td>\n",
       "      <td>October 25, 1791</td>\n",
       "      <td>November 6, 1792</td>\n",
       "      <td>December 3, 1793</td>\n",
       "      <td>November 19, 1794</td>\n",
       "      <td>December 8, 1795</td>\n",
       "      <td>December 7, 1796</td>\n",
       "      <td>November 22, 1797</td>\n",
       "      <td>December 8, 1798</td>\n",
       "      <td>December 3, 1799</td>\n",
       "      <td>...</td>\n",
       "      <td>February 24, 2009</td>\n",
       "      <td>January 27, 2010</td>\n",
       "      <td>January 25, 2011</td>\n",
       "      <td>January 24, 2012</td>\n",
       "      <td>February 12, 2013</td>\n",
       "      <td>January 28, 2014</td>\n",
       "      <td>January 20, 2015</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>February 28, 2017</td>\n",
       "      <td>January 30, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>...</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "      <td>Madame Speaker, Vice President Biden, Members ...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1790  \\\n",
       "date                                     January 8, 1790   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1791  \\\n",
       "date                                    October 25, 1791   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1792  \\\n",
       "date                                    November 6, 1792   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1793  \\\n",
       "date                                    December 3, 1793   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1794  \\\n",
       "date                                   November 19, 1794   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1795  \\\n",
       "date                                    December 8, 1795   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1796  \\\n",
       "date                                    December 7, 1796   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1797  \\\n",
       "date                                   November 22, 1797   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1798  \\\n",
       "date                                    December 8, 1798   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1799  \\\n",
       "date                                    December 3, 1799   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                             ...                          \\\n",
       "date                         ...                           \n",
       "name                         ...                           \n",
       "texts                        ...                           \n",
       "\n",
       "                                                    2009  \\\n",
       "date                                   February 24, 2009   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Mr. Vice President, Members of...   \n",
       "\n",
       "                                                    2010  \\\n",
       "date                                    January 27, 2010   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Vice President Biden, Members ...   \n",
       "\n",
       "                                                    2011  \\\n",
       "date                                    January 25, 2011   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2012  \\\n",
       "date                                    January 24, 2012   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2013  \\\n",
       "date                                   February 12, 2013   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2014  \\\n",
       "date                                    January 28, 2014   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2015  \\\n",
       "date                                    January 20, 2015   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2016  \\\n",
       "date                                    January 12, 2016   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2017  \\\n",
       "date                                   February 28, 2017   \n",
       "name                                     Donald J. Trump   \n",
       "texts  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "\n",
       "                                                    2018  \n",
       "date                                    January 30, 2018  \n",
       "name                                     Donald J. Trump  \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('sotu'):\n",
    "    with open('sotu/'+filename) as fin, open('sotu-clean/'+filename, 'w') as fout:\n",
    "        fout.write(re.sub('<[^<]+?>', '', fin.read()).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City.db\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sem.chat80 import cities2table, sql_query\n",
    "from sqlite3 import OperationalError\n",
    "try:\n",
    "    cities2table('cities.pl', 'city', 'city.db', verbose=True, setup=True)\n",
    "except OperationalError:\n",
    "    pass \n",
    "\n",
    "directory = new_nltk_data+'/corpora/city_database/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with open(directory+'city.tsv', 'w') as fout:\n",
    "    for row in sql_query('corpora/city_database/city.db', \"SELECT * FROM city_table\"):\n",
    "        city, country, population = row\n",
    "        print('\\t'.join([city, country, str(population)]), end='\\n', file=fout)\n",
    "    \n",
    "chat80_meta = {'title': 'Chat80', \n",
    "             'author': '', \n",
    "             'source': '',\n",
    "            }\n",
    "\n",
    "\n",
    "directory = new_nltk_data+'/corpora/chat80/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "# contain.pl\n",
    "with open(directory+'contain.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/contain.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'^contains0\\((.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                country, contains = matches[0]\n",
    "                print('\\t'.join([country, contains]), end='\\n', file=fout)\n",
    "\n",
    "\n",
    "# borders.pl\n",
    "with open(directory+'borders.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/borders.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'borders\\((.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                query, bordering = matches[0]\n",
    "                print('\\t'.join([query, bordering]), end='\\n', file=fout)\n",
    "                \n",
    "# cities.pl\n",
    "with open(directory+'cities.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/cities.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'city\\((.*),(.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                city, country, population = matches[0]\n",
    "                print('\\t'.join([city, country, population]), end='\\n', file=fout)\n",
    "                \n",
    "# countries.pl\n",
    "with open(directory+'countries.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/countries.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'country\\((.*),(.*),(.*),(.*),(.*),(.*),(.*),(.*)\\)\\.$', line)\n",
    "            if matches:\n",
    "                country, region, latitude, longtitude, area, population, capital, currency = matches[0]\n",
    "                print('\\t'.join(matches[0]), end='\\n', file=fout)\n",
    "\n",
    "# rivers.pl \n",
    "with open(directory+'rivers.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/rivers.pl') as fin:\n",
    "        for line in fin:\n",
    "            matches = re.findall(r'river\\((.*),\\[(.*)\\]\\)\\.$', line)\n",
    "            if matches:\n",
    "                river, flows_thru = matches[0]\n",
    "                print('\\t'.join([river, str(flows_thru.split(','))]), file=fout, end='\\n')\n",
    "            \n",
    "# world1.pl\n",
    "with open(directory+'world1-circle-of-latitutde.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/world1.pl') as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('circle_of_latitude'):\n",
    "                matches = re.findall(r'(circle_of_latitude)\\((.*),(.*)\\)\\.', line)\n",
    "                rel, circle, number = matches[0]\n",
    "                print('\\t'.join(['circle_of_latitude', circle, number]), end='\\n', file=fout)\n",
    "                \n",
    "with open(directory+'world1-in-continent.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/world1.pl') as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('in_continent'):\n",
    "                matches = re.findall(r'in_continent\\((.*)\\,(.*)\\)\\.', line)\n",
    "                region, continent  = matches[0]\n",
    "                print('\\t'.join(['in_continent', region, continent]), end='\\n', file=fout)\n",
    "                \n",
    "with open(directory+'world1-continent-ocean-sea.tsv', 'w') as fout:\n",
    "    with open(old_nltk_data + '/corpora/chat80/world1.pl') as fin:\n",
    "        for line in fin:\n",
    "            if not line.startswith('in_continent'):\n",
    "                matches = re.findall(r'(continent|ocean|sea)\\((.*)\\)\\.', line)\n",
    "                if matches:\n",
    "                    rel, entity = matches[0]\n",
    "                    print('\\t'.join([rel, entity]), end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dolch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyme import find_files\n",
    "\n",
    "dolch_meta = {'title':'Dolch Word List',\n",
    "            'description': str(\n",
    "                           \"This corpus contains a list of frequently used English words, grouped according to their part of speech.\"\n",
    "                           \"These are 220 sight words that make up most of children's reading materials.\"),\n",
    "            'cite': 'Dolch, E. W. (1936). A basic sight vocabulary. The Elementary School Journal, 36(6), 456--460.',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/dolch/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(directory+'dolch.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['pos', 'word']), end='\\n', file=fout)\n",
    "    for filename in find_files(old_nltk_data+'/corpora/dolch/', '*'):\n",
    "        if not filename.lower().endswith('readme'):\n",
    "            with open(filename) as fin:\n",
    "                pos = filename.split('/')[-1]\n",
    "                for line in fin:\n",
    "                    print('\\t'.join([pos, line.strip()]), end='\\n', file=fout)\n",
    "\n",
    "with open(new_nltk_data+'/corpora/dolch/dolch-meta.json', 'w') as fout:\n",
    "    json.dump(dolch_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comtrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazyme import find_files, per_chunk\n",
    "\n",
    "format_description = \"\"\"The data is in giza++ format, consisting of triples of\n",
    "L1, L2, and alignments, e.g.:\n",
    "\n",
    "English-French:\n",
    "Resumption of the session\n",
    "Reprise de la session\n",
    "0-0 1-1 2-2 3-3\n",
    "\n",
    "German-English:\n",
    "Wiederaufnahme der Sitzungsperiode\n",
    "Resumption of the session\n",
    "0-0 1-1 1-2 2-3\n",
    "\n",
    "German-French:\n",
    "Wiederaufnahme der Sitzungsperiode\n",
    "Reprise de la session\n",
    "0-0 1-1 1-2 2-3\"\"\"\n",
    "\n",
    "comtrans_meta = {'title':'COMTRANS Corpus Sample',\n",
    "            'description': str(\n",
    "                           \"3.3% of the COMTRANS data, distributed with permission.\\n\\n\"\n",
    "                           )+format_description,\n",
    "            'authors': 'Reinhard Rapp',\n",
    "            'source': 'http://www.fask.uni-mainz.de/user/rapp/comtrans/',\n",
    "            'cite': '',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/comtrans/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/comtrans/comtrans-meta.json', 'w') as fout:\n",
    "    json.dump(comtrans_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + 'comtrans-sample.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['filename', \n",
    "                     'src_lang', 'trg_lang', 'idx', \n",
    "                    'src', 'trg', 'alignment']), file=fout, end='\\n')\n",
    "    \n",
    "    for filename in find_files(old_nltk_data + '/corpora/comtrans/', '*.txt'):\n",
    "        x = filename.split('/')[-1].split('.')[0].split('-')\n",
    "        _, src, trg = x\n",
    "        with open(filename, encoding='latin-1') as fin:\n",
    "\n",
    "            for idx, three_lines in enumerate(per_chunk(fin, n=3)):\n",
    "                srcline, trgline, align = three_lines\n",
    "                srcline = srcline.strip().encode('utf-8').decode('utf-8')\n",
    "                trgline = trgline.strip().encode('utf-8').decode('utf-8')\n",
    "                align = align.strip().encode('utf-8').decode('utf-8')\n",
    "                print('\\t'.join([filename.split('/')[-1], \n",
    "                                 src, trg, str(idx), \n",
    "                                 srcline, trgline, align]),\n",
    "                      file=fout, end='\\n'\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + 'comtrans-full.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['filename', \n",
    "                     'src_lang', 'trg_lang', 'idx', \n",
    "                    'src', 'trg', 'alignment']), file=fout, end='\\n')\n",
    "    \n",
    "    for filename in find_files(old_nltk_data + '/corpora/comtrans-full/', '*.txt'):\n",
    "        x = filename.split('/')[-1].split('.')[0].split('-')\n",
    "        _, _, src, trg, _  = x\n",
    "        with open(filename, encoding='latin-1') as fin:\n",
    "\n",
    "            for idx, three_lines in enumerate(per_chunk(fin, n=3)):\n",
    "                srcline, trgline, align = three_lines\n",
    "                srcline = srcline.strip().encode('utf-8').decode('utf-8')\n",
    "                trgline = trgline.strip().encode('utf-8').decode('utf-8')\n",
    "                align = align.strip().encode('utf-8').decode('utf-8')\n",
    "                print('\\t'.join([filename.split('/')[-1], \n",
    "                                 src, trg, str(idx), \n",
    "                                 srcline, trgline, align]),\n",
    "                      file=fout, end='\\n'\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crubadan\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "crubadan_readme = \"\"\"\n",
    "Language Id Corpus\n",
    "Kevin Scannell\n",
    "\n",
    "This directory contains 3-gram frequencies for 449 writing systems \n",
    "gathered by the web crawler \"An Crúbadán\", as of 11 April 2010.\n",
    "See http://borel.slu.edu/crubadan/ for more information.\n",
    "\n",
    "The web crawler works at the level of \"writing systems\" vs. \"languages\",\n",
    "so for example Serbian Cyrillic and Serbian Latin are treated\n",
    "separately, as are Portuguese as spoken in Brazil vs. Portugal, etc.\n",
    "The 3-gram files are named using 2- or 3-letter \"writing system codes\"\n",
    "that were never intended to be exposed to the outside world.\n",
    "We are working on establishing a mapping between our codes and\n",
    "the writing systems laid out in Oliver Streiter's XNL-RDF database.  \n",
    "\n",
    "The file table.txt lists all 449 writing systems.  The first column\n",
    "contains the internal Crúbadán code, the second column contains the\n",
    "ISO 639-3 code for the language represented by the writing system, and\n",
    "the third column is an English language description.\n",
    "\n",
    "Copyright 2010 Kevin P. Scannell <kscanne at gmail dot com>\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "crubadan_meta = {'title':'Crúbadán Language Id Corpus',\n",
    "            'description': str(\n",
    "                            \"This directory contains 3-gram frequencies for 449 writing systems\\n\"\n",
    "                            'gathered by the web crawler \"An Crúbadán\", as of 11 April 2010.\\n'\n",
    "                            \"See http://borel.slu.edu/crubadan/ for more information.\\n\"\n",
    "                           ),\n",
    "            'readme': crubadan_readme,\n",
    "            'authors': 'Kevin Scannell',\n",
    "            'source': 'http://borel.slu.edu/crubadan',\n",
    "            'cite': '',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/crubadan/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/crubadan/crubadan-meta.json', 'w') as fout:\n",
    "    json.dump(crubadan_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    \n",
    "    \n",
    "language_mappings = []\n",
    "iso6392_to_iso6393 = {}\n",
    "with open(old_nltk_data+'/corpora/crubadan/table.txt') as fin:\n",
    "    for line in fin:\n",
    "        iso6392, iso6393, lang = line.strip().split('\\t')\n",
    "        language_mappings.append({'iso639-2':iso6392, 'iso639-3':iso6393, 'language':lang})\n",
    "        iso6392_to_iso6393[iso6392] = iso6393\n",
    "df_lang = pd.DataFrame.from_dict(language_mappings)  \n",
    "\n",
    "df_lang.to_csv(new_nltk_data+'/corpora/crubadan/language_mapping.tsv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "trigram_counter = defaultdict(Counter)\n",
    "for filename in find_files(old_nltk_data+'/corpora/crubadan/', '*3grams.txt'):\n",
    "    lang = iso6392_to_iso6393[filename.split('/')[-1].split('-')[0]]\n",
    "    with open(filename) as fin:\n",
    "        for line in fin:\n",
    "            count, gram = line.strip().split(' ')\n",
    "            trigram_counter[lang][gram] = int(count)\n",
    "            rows.append({'lang':lang, 'trigram':gram, 'count':int(count)})\n",
    "            \n",
    "df_crubadan = pd.DataFrame.from_dict(rows)\n",
    "\n",
    "df_crubadan.to_csv(new_nltk_data +'/corpora/crubadan/crubadan.tsv', sep='\\t', index=False)\n",
    "\n",
    "with open(new_nltk_data +'/corpora/crubadan/crubadan.pkl', 'wb') as fout:\n",
    "    pickle.dump(trigram_counter, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import machado\n",
    "from nltk.data import LazyLoader\n",
    "\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *\n",
    "\n",
    "def _read_para_block(stream):\n",
    "    paras = []\n",
    "    for para in read_blankline_block(stream):\n",
    "        paras.append(\n",
    "            [\n",
    "                sent.replace('\\n', ' ')\n",
    "                for sent in sent_tokenize_pt(para)\n",
    "            ]\n",
    "        )\n",
    "    return paras\n",
    "\n",
    "def machado_paras(fileid):\n",
    "    return concat(\n",
    "                [\n",
    "                    machado.CorpusView(path, _read_para_block, encoding=enc)\n",
    "                    for (path, enc, fileid) in machado.abspaths(fileid, True, True)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "sent_tokenize_pt = LazyLoader(\"tokenizers/punkt/portuguese.pickle\").tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:16<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(new_nltk_data +'/corpora/machado/machado.tsv', 'w') as fout:\n",
    "    print('\\t'.join(['genre', 'filename', 'para_idx', 'sent_idx', 'sent']), end='\\n', file=fout)\n",
    "    for filepath in tqdm(machado.fileids()):\n",
    "        genre, filename = filepath.split('/')\n",
    "        for para_idx, para in enumerate(machado_paras(filepath)):\n",
    "            for sent_idx, sent in enumerate(para):\n",
    "                print('\\t'.join(map(str, [genre, filename, para_idx, sent_idx, sent])), end='\\n', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machado_meta = {'title':'Machado de Assis -- Obra Completa',\n",
    "            'description': str(\n",
    "                            \"This directory contains 3-gram frequencies for 449 writing systems\\n\"\n",
    "                            'gathered by the web crawler \"An Crúbadán\", as of 11 April 2010.\\n'\n",
    "                            \"See http://borel.slu.edu/crubadan/ for more information.\\n\"\n",
    "                           ),\n",
    "            'readme': '',\n",
    "            'authors': '',\n",
    "            'source': 'http://machado.mec.gov.br',\n",
    "            'cite': '',\n",
    "           }\n",
    "\n",
    "directory = new_nltk_data+'/corpora/machado/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "with open(new_nltk_data+'/corpora/machado/machado-meta.json', 'w') as fout:\n",
    "    json.dump(crubadan_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_reports_meta = {'title':'Problem Report Corpus',\n",
    "            'description': str(\n",
    "                           \"3.3% of the COMTRANS data, distributed with permission.\\n\\n\"\n",
    "                           )+format_description,\n",
    "            'authors': 'Reinhard Rapp',\n",
    "            'source': 'http://www.fask.uni-mainz.de/user/rapp/comtrans/',\n",
    "            'cite': '',\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
