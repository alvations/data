{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_nltk_data = nltk.data.path[0]\n",
    "\n",
    "new_nltk_data = \"packages/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC corpus.\n",
    "directory = new_nltk_data+'/corpora/abc/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with io.open(old_nltk_data+'/corpora/abc/rural.txt') as fin:\n",
    "    rural_texts = [line.strip() for line in fin if line.strip()]\n",
    "with io.open(old_nltk_data+'/corpora/abc/science.txt', encoding='latin_1') as fin:\n",
    "    science_texts = [line.strip().encode('utf8').decode('utf8') for line in fin if \n",
    "                    line.strip().encode('utf8').decode('utf8')]\n",
    "\n",
    "rural_df = pd.DataFrame({'text':rural_texts})\n",
    "rural_df['subcorpora'] = 'Rural News'\n",
    "\n",
    "science_df = pd.DataFrame({'text':science_texts})\n",
    "science_df['subcorpora'] = 'Science News'\n",
    "\n",
    "df_abc = pd.concat([rural_df, science_df])\n",
    "df_abc.to_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', index=False)\n",
    "df_abc = pd.read_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'subcorpora':str})\n",
    "\n",
    "abc_meta = {'title':'Australian Broadcasting Commission 2006',\n",
    "            'source': 'http://www.abc.net.au/',\n",
    "            'subcorpora': {'Rural News': {'source': 'http://www.abc.net.au/rural/news/'},\n",
    "                           'Science News': {'source': 'http://www.abc.net.au/science/news/'}\n",
    "                          },\n",
    "             'xml': {'id':'abc', 'name':\"Australian Broadcasting Commission 2006\",\n",
    "                     'webpage':\"http://www.abc.net.au/\", 'author':\"Australian Broadcasting Commission\",\n",
    "                      'unzip':\"1\"}\n",
    "           }\n",
    "\n",
    "\n",
    "abc_xml = ET.Element(\"package\", id=\"abc\", name=\"Australian Broadcasting Commission 2006\",\n",
    "                  webpage=\"http://www.abc.net.au/\", author=\"Australian Broadcasting Commission\",\n",
    "                  unzip=\"1\")\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(new_nltk_data+'/corpora/abc/abc.xml')\n",
    "\n",
    "with open(new_nltk_data+'/corpora/abc/abc-meta.json', 'w') as fout:\n",
    "    json.dump(abc_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subcorpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PM denies knowledge of AWB kickbacks</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Prime Minister has denied he knew AWB was ...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Letters from John Howard and Deputy Prime Mini...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In one of the letters Mr Howard asks AWB manag...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Opposition's Gavan O'Connor says the lette...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  subcorpora\n",
       "0               PM denies knowledge of AWB kickbacks  Rural News\n",
       "1  The Prime Minister has denied he knew AWB was ...  Rural News\n",
       "2  Letters from John Howard and Deputy Prime Mini...  Rural News\n",
       "3  In one of the letters Mr Howard asks AWB manag...  Rural News\n",
       "4  The Opposition's Gavan O'Connor says the lette...  Rural News"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown\n",
    "\n",
    "directory = new_nltk_data+'/corpora/brown/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "    \n",
    "with open(old_nltk_data+'/corpora/brown/cats.txt') as fin:\n",
    "     categories = {line.strip().split(' ')[0]:line.strip().split(' ')[1] \n",
    "                   for line in fin}\n",
    "        \n",
    "brown_dir = old_nltk_data+'/corpora/brown/'\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(brown_dir):\n",
    "    if filename in ['CONTENTS', 'cats.txt', 'README']:\n",
    "        continue\n",
    "    cat = categories[filename]\n",
    "    with open(brown_dir+filename) as fin:\n",
    "        i = -1\n",
    "        for paragraph in fin.read().split('\\n\\n'):\n",
    "            if not paragraph.strip():\n",
    "                continue\n",
    "            i += 1\n",
    "            j = -1\n",
    "            for sent in paragraph.split('\\n'):\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                j += 1\n",
    "                raw = sent.strip()\n",
    "                text, pos = zip(*[word.split('/') for word in raw.split()])\n",
    "                rows.append({'filename': filename, \n",
    "                              'para_id': i, \n",
    "                              'sent_id': j, \n",
    "                              'raw_text': raw, \n",
    "                              'tokenized_text': ' '.join(text), \n",
    "                              'tokenized_pos': ' '.join(pos), \n",
    "                              'label': cat})\n",
    "                \n",
    "                \n",
    "df_brown = pd.DataFrame(rows)[['filename', 'para_id', 'sent_id', \n",
    "                              'raw_text', 'tokenized_text', 'tokenized_pos', 'label']]\n",
    "df_brown.to_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', index=False)\n",
    "\n",
    "df_brown = pd.read_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', \n",
    "                     dtype={'filename':str, 'para_id':int, 'sent_id':int,\n",
    "                             'raw_text':str, 'tokenized_text':str, 'tokenized_pos':str,\n",
    "                           'label':str})\n",
    "\n",
    "df_brown_cats = df_brown[['filename', 'label']].drop_duplicates().sort_values('filename')\n",
    "df_brown_cats.to_csv(new_nltk_data+'/corpora/brown/cats.tsv', sep='\\t', index=False)\n",
    "\n",
    "brown_readme = \"\"\"BROWN CORPUS\n",
    "\n",
    "A Standard Corpus of Present-Day Edited American\n",
    "English, for use with Digital Computers.\n",
    "\n",
    "by W. N. Francis and H. Kucera (1964)\n",
    "Department of Linguistics, Brown University\n",
    "Providence, Rhode Island, USA\n",
    "\n",
    "Revised 1971, Revised and Amplified 1979\n",
    "\n",
    "http://www.hit.uib.no/icame/brown/bcm.html\n",
    "\n",
    "Distributed with the permission of the copyright holder,\n",
    "redistribution permitted.\"\"\"\n",
    "\n",
    "brown_meta = {'title':'Brown Corpus',\n",
    "              'description': str('A Standard Corpus of Present-Day Edited American English, '\n",
    "                               'for use with Digital Computers.'),\n",
    "              'authors': 'W. N. Francis and H. Kucera (1964)',\n",
    "              'url': 'http://www.hit.uib.no/icame/brown/bcm.html',\n",
    "              'readme': brown_readme}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/brown/brown-meta.json', 'w') as fout:\n",
    "    json.dump(brown_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore/rb ,/, as/cs an/at encouragement/n...</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "      <td>rb , cs at nn in nn nn , pps rb bez jj to vb c...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The/at Unitarian/jj clergy/nns were/bed an/at ...</td>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "      <td>at jj nns bed at jj nn in vbn nns -- cs at nn ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Ezra/np Stiles/np Gannett/np ,/, an/at honorab...</td>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "      <td>np np np , at jj nn in at nn , vbd ppl rb in a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Even/rb so/rb ,/, Gannett/np judiciously/rb ar...</td>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "      <td>rb rb , np rb vbd , at nn-tl md rb vb cs np ``...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>We/ppss today/nr are/ber not/* entitled/vbn to...</td>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "      <td>ppss nr ber * vbn to vb jj nns wps vbd np to b...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  para_id  sent_id  \\\n",
       "0     cd05        0        0   \n",
       "1     cd05        0        1   \n",
       "2     cd05        0        2   \n",
       "3     cd05        0        3   \n",
       "4     cd05        0        4   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  Furthermore/rb ,/, as/cs an/at encouragement/n...   \n",
       "1  The/at Unitarian/jj clergy/nns were/bed an/at ...   \n",
       "2  Ezra/np Stiles/np Gannett/np ,/, an/at honorab...   \n",
       "3  Even/rb so/rb ,/, Gannett/np judiciously/rb ar...   \n",
       "4  We/ppss today/nr are/ber not/* entitled/vbn to...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  Furthermore , as an encouragement to revisioni...   \n",
       "1  The Unitarian clergy were an exclusive club of...   \n",
       "2  Ezra Stiles Gannett , an honorable representat...   \n",
       "3  Even so , Gannett judiciously argued , the Ass...   \n",
       "4  We today are not entitled to excoriate honest ...   \n",
       "\n",
       "                                       tokenized_pos     label  \n",
       "0  rb , cs at nn in nn nn , pps rb bez jj to vb c...  religion  \n",
       "1  at jj nns bed at jj nn in vbn nns -- cs at nn ...  religion  \n",
       "2  np np np , at jj nn in at nn , vbd ppl rb in a...  religion  \n",
       "3  rb rb , np rb vbd , at nn-tl md rb vb cs np ``...  religion  \n",
       "4  ppss nr ber * vbn to vb jj nns wps vbd np to b...  religion  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazetteers\n",
    "\n",
    "directory = new_nltk_data+'/corpora/gazetteers/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(old_nltk_data+'/corpora/gazetteers/'):\n",
    "    if filename in ['LICENSE.txt']:\n",
    "        continue\n",
    "    label = gazetteers_filename2labels[filename]\n",
    "    with io.open(old_nltk_data+'/corpora/gazetteers/'+filename, encoding='ISO-8859-2') as fin:\n",
    "        for line in fin:\n",
    "            if line.strip():\n",
    "                text = line.strip()\n",
    "                if text == 'QuerĂŠtaro':\n",
    "                    text = 'Querétaro'\n",
    "                rows.append({'text':text, 'label':label})\n",
    "\n",
    "df_gazetteers = pd.DataFrame(rows)[['text', 'label']]\n",
    "\n",
    "#alpabet = list('abcdefghijklmnopqrstuvwxyz. ()-,') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "#alpabet += [\"'\"]\n",
    "#[word for word in df_gazetteers['text'] if any(ch for ch in word if ch not in alpabet)]\n",
    "\n",
    "df_gazetteers.to_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', index=False)\n",
    "df_gazetteers = pd.read_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'label':str})\n",
    "\n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "gazetteers_meta = {'title':'Geolocation Gazeteers',\n",
    "                    'subcorpora': {'Mexico States': {'original_file': 'mexstates.txt'},\n",
    "                                   'Canada Provinces': {'original_file': 'caprovinces.txt'},\n",
    "                                   'US State Abbreviations': {'original_file': 'usstates.txt'},\n",
    "                                   'US States': {'original_file': 'usstateabbrev.txt'},\n",
    "                                   'US Cities': {'original_file':'uscities.txt',\n",
    "                                                 'source': 'http://en.wikipedia.org/wiki/List_of_cities_in_the_United_States_with_over_100%2C000_people',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries': {'original_file':'countries.txt',\n",
    "                                                 'source':'http://en.wikipedia.org/wiki/List_of_countries',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries ISO codes': {'original_file': 'isocountries.txt',\n",
    "                                                          'source': 'http://www.guavastudios.com/country-list.htm'\n",
    "                                                          },\n",
    "                                   'Nationalities': {'original_file': 'nationalities.txt',\n",
    "                                                    'source': 'http://www.guavastudios.com/nationalities-list.htm'\n",
    "                                                    },\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/gazetteers/gazetteers-meta.json', 'w') as fout:\n",
    "    json.dump(gazetteers_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "directory = new_nltk_data+'/corpora/words/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en') as fin:\n",
    "    for line in fin:\n",
    "        en_words.append(line.strip())\n",
    "\n",
    "basic_en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en-basic') as fin:\n",
    "    for line in fin:\n",
    "        basic_en_words.append(line.strip())\n",
    "        \n",
    "words_meta = {'title':'Word Lists',\n",
    "              'subcorpora': {'Unix Words':{'source':'http://en.wikipedia.org/wiki/Words_(Unix)'},\n",
    "                           'Ogden Basic English': {'title': 'The ABC of Basic English',\n",
    "                                                   'author':'C.K. Ogden (1932)'}\n",
    "                          }\n",
    "            }\n",
    "\n",
    "unix_words = pd.DataFrame({'text':en_words})\n",
    "ogden_words = pd.DataFrame({'text':basic_en_words})\n",
    "\n",
    "unix_words.to_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', index=False)\n",
    "ogden_words.to_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', index=False)\n",
    "\n",
    "unix_words = pd.read_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', dtype={'text':str})\n",
    "ogden_words = pd.read_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', dtype={'text':str})\n",
    "\n",
    "with open(new_nltk_data+'/corpora/words/words-meta.json', 'w') as fout:\n",
    "    json.dump(words_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = new_nltk_data+'/corpora/movie_reviews/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/pos/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "    \n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/pos/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'pos'\n",
    "                            })\n",
    "                \n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/neg/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "\n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/neg/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'neg'\n",
    "                            })\n",
    "                \n",
    "df_movie_reivews = pd.DataFrame(rows)[['fold_id', 'cv_tag', 'html_id', 'sent_id', 'text', 'tag']]\n",
    "\n",
    "df_movie_reivews.to_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', index=False)\n",
    "df_movie_reivews = pd.read_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', \n",
    "                     dtype={'fold_id':int, 'cv_tag':str, 'html_id':str, 'sent_id':int,\n",
    "                            'text':str, 'tag':str})\n",
    "\n",
    "\n",
    "mr_bibtext = \"\"\"@InProceedings{Pang+Lee:04a,\n",
    "  author =       {Bo Pang and Lillian Lee},\n",
    "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
    "  booktitle =    \"Proceedings of the ACL\",\n",
    "  year =         2004\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "mr_meta = {'title': 'Sentiment Polarity Dataset Version 2.0',\n",
    "           'aka': 'Moview Review Data',\n",
    "           'source': 'http://www.cs.cornell.edu/people/pabo/movie-review-data/',\n",
    "           'authors': 'Bo Pang and Lillian Lee',\n",
    "           'license': 'Distributed with NLTK with permission from the authors.',\n",
    "            'bibtex':mr_bibtext}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/movie_reviews/movie_reviews-meta.json', 'w') as fout:\n",
    "    json.dump(mr_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
