{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from shutil import copyfile\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from lazyme import per_section, deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_nltk_data = nltk.data.path[0]\n",
    "\n",
    "new_nltk_data = \"packages/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABC Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABC corpus.\n",
    "directory = new_nltk_data+'/corpora/abc/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "with io.open(old_nltk_data+'/corpora/abc/rural.txt') as fin:\n",
    "    rural_texts = [line.strip() for line in fin if line.strip()]\n",
    "with io.open(old_nltk_data+'/corpora/abc/science.txt', encoding='latin_1') as fin:\n",
    "    science_texts = [line.strip().encode('utf8').decode('utf8') for line in fin if \n",
    "                    line.strip().encode('utf8').decode('utf8')]\n",
    "\n",
    "rural_df = pd.DataFrame({'text':rural_texts})\n",
    "rural_df['subcorpora'] = 'Rural News'\n",
    "\n",
    "science_df = pd.DataFrame({'text':science_texts})\n",
    "science_df['subcorpora'] = 'Science News'\n",
    "\n",
    "df_abc = pd.concat([rural_df, science_df])\n",
    "df_abc.to_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', index=False)\n",
    "df_abc = pd.read_csv(new_nltk_data+'/corpora/abc/abc.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'subcorpora':str})\n",
    "\n",
    "abc_meta = {'title':'Australian Broadcasting Commission 2006',\n",
    "            'source': 'http://www.abc.net.au/',\n",
    "            'subcorpora': {'Rural News': {'source': 'http://www.abc.net.au/rural/news/'},\n",
    "                           'Science News': {'source': 'http://www.abc.net.au/science/news/'}\n",
    "                          },\n",
    "             'xml': {'id':'abc', 'name':\"Australian Broadcasting Commission 2006\",\n",
    "                     'webpage':\"http://www.abc.net.au/\", 'author':\"Australian Broadcasting Commission\",\n",
    "                      'unzip':\"1\"}\n",
    "           }\n",
    "\n",
    "\n",
    "abc_xml = ET.Element(\"package\", id=\"abc\", name=\"Australian Broadcasting Commission 2006\",\n",
    "                  webpage=\"http://www.abc.net.au/\", author=\"Australian Broadcasting Commission\",\n",
    "                  unzip=\"1\")\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(new_nltk_data+'/corpora/abc/abc.xml')\n",
    "\n",
    "with open(new_nltk_data+'/corpora/abc/abc-meta.json', 'w') as fout:\n",
    "    json.dump(abc_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subcorpora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PM denies knowledge of AWB kickbacks</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Prime Minister has denied he knew AWB was ...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Letters from John Howard and Deputy Prime Mini...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In one of the letters Mr Howard asks AWB manag...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Opposition's Gavan O'Connor says the lette...</td>\n",
       "      <td>Rural News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  subcorpora\n",
       "0               PM denies knowledge of AWB kickbacks  Rural News\n",
       "1  The Prime Minister has denied he knew AWB was ...  Rural News\n",
       "2  Letters from John Howard and Deputy Prime Mini...  Rural News\n",
       "3  In one of the letters Mr Howard asks AWB manag...  Rural News\n",
       "4  The Opposition's Gavan O'Connor says the lette...  Rural News"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown\n",
    "\n",
    "directory = new_nltk_data+'/corpora/brown/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "    \n",
    "with open(old_nltk_data+'/corpora/brown/cats.txt') as fin:\n",
    "     categories = {line.strip().split(' ')[0]:line.strip().split(' ')[1] \n",
    "                   for line in fin}\n",
    "        \n",
    "brown_dir = old_nltk_data+'/corpora/brown/'\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(brown_dir):\n",
    "    if filename in ['CONTENTS', 'cats.txt', 'README']:\n",
    "        continue\n",
    "    cat = categories[filename]\n",
    "    with open(brown_dir+filename) as fin:\n",
    "        i = -1\n",
    "        for paragraph in fin.read().split('\\n\\n'):\n",
    "            if not paragraph.strip():\n",
    "                continue\n",
    "            i += 1\n",
    "            j = -1\n",
    "            for sent in paragraph.split('\\n'):\n",
    "                if not sent.strip():\n",
    "                    continue\n",
    "                j += 1\n",
    "                raw = sent.strip()\n",
    "                text, pos = zip(*[word.split('/') for word in raw.split()])\n",
    "                rows.append({'filename': filename, \n",
    "                              'para_id': i, \n",
    "                              'sent_id': j, \n",
    "                              'raw_text': raw, \n",
    "                              'tokenized_text': ' '.join(text), \n",
    "                              'tokenized_pos': ' '.join(pos), \n",
    "                              'label': cat})\n",
    "                \n",
    "                \n",
    "df_brown = pd.DataFrame(rows)[['filename', 'para_id', 'sent_id', \n",
    "                              'raw_text', 'tokenized_text', 'tokenized_pos', 'label']]\n",
    "df_brown.to_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', index=False)\n",
    "\n",
    "df_brown = pd.read_csv(new_nltk_data+'/corpora/brown/brown.tsv', sep='\\t', \n",
    "                     dtype={'filename':str, 'para_id':int, 'sent_id':int,\n",
    "                             'raw_text':str, 'tokenized_text':str, 'tokenized_pos':str,\n",
    "                           'label':str})\n",
    "\n",
    "df_brown_cats = df_brown[['filename', 'label']].drop_duplicates().sort_values('filename')\n",
    "df_brown_cats.to_csv(new_nltk_data+'/corpora/brown/cats.tsv', sep='\\t', index=False)\n",
    "\n",
    "brown_readme = \"\"\"BROWN CORPUS\n",
    "\n",
    "A Standard Corpus of Present-Day Edited American\n",
    "English, for use with Digital Computers.\n",
    "\n",
    "by W. N. Francis and H. Kucera (1964)\n",
    "Department of Linguistics, Brown University\n",
    "Providence, Rhode Island, USA\n",
    "\n",
    "Revised 1971, Revised and Amplified 1979\n",
    "\n",
    "http://www.hit.uib.no/icame/brown/bcm.html\n",
    "\n",
    "Distributed with the permission of the copyright holder,\n",
    "redistribution permitted.\"\"\"\n",
    "\n",
    "brown_meta = {'title':'Brown Corpus',\n",
    "              'description': str('A Standard Corpus of Present-Day Edited American English, '\n",
    "                               'for use with Digital Computers.'),\n",
    "              'authors': 'W. N. Francis and H. Kucera (1964)',\n",
    "              'url': 'http://www.hit.uib.no/icame/brown/bcm.html',\n",
    "              'readme': brown_readme}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/brown/brown-meta.json', 'w') as fout:\n",
    "    json.dump(brown_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Furthermore/rb ,/, as/cs an/at encouragement/n...</td>\n",
       "      <td>Furthermore , as an encouragement to revisioni...</td>\n",
       "      <td>rb , cs at nn in nn nn , pps rb bez jj to vb c...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The/at Unitarian/jj clergy/nns were/bed an/at ...</td>\n",
       "      <td>The Unitarian clergy were an exclusive club of...</td>\n",
       "      <td>at jj nns bed at jj nn in vbn nns -- cs at nn ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Ezra/np Stiles/np Gannett/np ,/, an/at honorab...</td>\n",
       "      <td>Ezra Stiles Gannett , an honorable representat...</td>\n",
       "      <td>np np np , at jj nn in at nn , vbd ppl rb in a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Even/rb so/rb ,/, Gannett/np judiciously/rb ar...</td>\n",
       "      <td>Even so , Gannett judiciously argued , the Ass...</td>\n",
       "      <td>rb rb , np rb vbd , at nn-tl md rb vb cs np ``...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd05</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>We/ppss today/nr are/ber not/* entitled/vbn to...</td>\n",
       "      <td>We today are not entitled to excoriate honest ...</td>\n",
       "      <td>ppss nr ber * vbn to vb jj nns wps vbd np to b...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  para_id  sent_id  \\\n",
       "0     cd05        0        0   \n",
       "1     cd05        0        1   \n",
       "2     cd05        0        2   \n",
       "3     cd05        0        3   \n",
       "4     cd05        0        4   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  Furthermore/rb ,/, as/cs an/at encouragement/n...   \n",
       "1  The/at Unitarian/jj clergy/nns were/bed an/at ...   \n",
       "2  Ezra/np Stiles/np Gannett/np ,/, an/at honorab...   \n",
       "3  Even/rb so/rb ,/, Gannett/np judiciously/rb ar...   \n",
       "4  We/ppss today/nr are/ber not/* entitled/vbn to...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  Furthermore , as an encouragement to revisioni...   \n",
       "1  The Unitarian clergy were an exclusive club of...   \n",
       "2  Ezra Stiles Gannett , an honorable representat...   \n",
       "3  Even so , Gannett judiciously argued , the Ass...   \n",
       "4  We today are not entitled to excoriate honest ...   \n",
       "\n",
       "                                       tokenized_pos     label  \n",
       "0  rb , cs at nn in nn nn , pps rb bez jj to vb c...  religion  \n",
       "1  at jj nns bed at jj nn in vbn nns -- cs at nn ...  religion  \n",
       "2  np np np , at jj nn in at nn , vbd ppl rb in a...  religion  \n",
       "3  rb rb , np rb vbd , at nn-tl md rb vb cs np ``...  religion  \n",
       "4  ppss nr ber * vbn to vb jj nns wps vbd np to b...  religion  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazetteers\n",
    "\n",
    "directory = new_nltk_data+'/corpora/gazetteers/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "rows = []\n",
    "for filename in os.listdir(old_nltk_data+'/corpora/gazetteers/'):\n",
    "    if filename in ['LICENSE.txt']:\n",
    "        continue\n",
    "    label = gazetteers_filename2labels[filename]\n",
    "    with io.open(old_nltk_data+'/corpora/gazetteers/'+filename, encoding='ISO-8859-2') as fin:\n",
    "        for line in fin:\n",
    "            if line.strip():\n",
    "                text = line.strip()\n",
    "                if text == 'QuerĂŠtaro':\n",
    "                    text = 'Querétaro'\n",
    "                rows.append({'text':text, 'label':label})\n",
    "\n",
    "df_gazetteers = pd.DataFrame(rows)[['text', 'label']]\n",
    "\n",
    "#alpabet = list('abcdefghijklmnopqrstuvwxyz. ()-,') + list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "#alpabet += [\"'\"]\n",
    "#[word for word in df_gazetteers['text'] if any(ch for ch in word if ch not in alpabet)]\n",
    "\n",
    "df_gazetteers.to_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', index=False)\n",
    "df_gazetteers = pd.read_csv(new_nltk_data + '/corpora/gazetteers/gazetteers.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'label':str})\n",
    "\n",
    "gazetteers_filename2labels = {'mexstates.txt':'Mexico States',\n",
    "                              'caprovinces.txt': 'Canada Provinces',\n",
    "                              'usstateabbrev.txt': 'US State Abbreviations',\n",
    "                              'uscities.txt': 'US Cities',\n",
    "                              'countries.txt': 'Countries',\n",
    "                              'isocountries.txt': 'Countries ISO codes',\n",
    "                              'nationalities.txt': 'Nationalities',\n",
    "                              'usstates.txt': 'US States'\n",
    "                             }\n",
    "\n",
    "gazetteers_meta = {'title':'Geolocation Gazeteers',\n",
    "                    'subcorpora': {'Mexico States': {'original_file': 'mexstates.txt'},\n",
    "                                   'Canada Provinces': {'original_file': 'caprovinces.txt'},\n",
    "                                   'US State Abbreviations': {'original_file': 'usstates.txt'},\n",
    "                                   'US States': {'original_file': 'usstateabbrev.txt'},\n",
    "                                   'US Cities': {'original_file':'uscities.txt',\n",
    "                                                 'source': 'http://en.wikipedia.org/wiki/List_of_cities_in_the_United_States_with_over_100%2C000_people',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries': {'original_file':'countries.txt',\n",
    "                                                 'source':'http://en.wikipedia.org/wiki/List_of_countries',\n",
    "                                                 'license': 'GNU Free Documentation License',\n",
    "                                                 'license_url': 'http://www.gnu.org/copyleft/fdl.html'\n",
    "                                                },\n",
    "                                   'Countries ISO codes': {'original_file': 'isocountries.txt',\n",
    "                                                          'source': 'http://www.guavastudios.com/country-list.htm'\n",
    "                                                          },\n",
    "                                   'Nationalities': {'original_file': 'nationalities.txt',\n",
    "                                                    'source': 'http://www.guavastudios.com/nationalities-list.htm'\n",
    "                                                    },\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/gazetteers/gazetteers-meta.json', 'w') as fout:\n",
    "    json.dump(gazetteers_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "directory = new_nltk_data+'/corpora/words/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en') as fin:\n",
    "    for line in fin:\n",
    "        en_words.append(line.strip())\n",
    "\n",
    "basic_en_words = []\n",
    "with open(old_nltk_data+'/corpora/words/en-basic') as fin:\n",
    "    for line in fin:\n",
    "        basic_en_words.append(line.strip())\n",
    "        \n",
    "words_meta = {'title':'Word Lists',\n",
    "              'subcorpora': {'Unix Words':{'source':'http://en.wikipedia.org/wiki/Words_(Unix)'},\n",
    "                           'Ogden Basic English': {'title': 'The ABC of Basic English',\n",
    "                                                   'author':'C.K. Ogden (1932)'}\n",
    "                          }\n",
    "            }\n",
    "\n",
    "unix_words = pd.DataFrame({'text':en_words})\n",
    "ogden_words = pd.DataFrame({'text':basic_en_words})\n",
    "\n",
    "unix_words.to_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', index=False)\n",
    "ogden_words.to_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', index=False)\n",
    "\n",
    "unix_words = pd.read_csv(new_nltk_data + '/corpora/words/unix_words.tsv', sep='\\t', dtype={'text':str})\n",
    "ogden_words = pd.read_csv(new_nltk_data + '/corpora/words/ogden_words.tsv', sep='\\t', dtype={'text':str})\n",
    "\n",
    "with open(new_nltk_data+'/corpora/words/words-meta.json', 'w') as fout:\n",
    "    json.dump(words_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = new_nltk_data+'/corpora/movie_reviews/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/pos/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "    \n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/pos/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'pos'\n",
    "                            })\n",
    "                \n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/movie_reviews/neg/')):\n",
    "    fold, html_id = filename[:-4].split('_')\n",
    "    fold_id = int(int(fold[2:]) / 100)\n",
    "\n",
    "    with open(old_nltk_data+'/corpora/movie_reviews/neg/'+filename) as fin:\n",
    "        for sent_id, line in enumerate(fin):\n",
    "                rows.append({'fold_id':fold_id, \n",
    "                             'cv_tag':fold, \n",
    "                             'html_id':html_id, \n",
    "                             'sent_id':sent_id, \n",
    "                             'text':line.strip(),\n",
    "                             'tag':'neg'\n",
    "                            })\n",
    "                \n",
    "df_movie_reivews = pd.DataFrame(rows)[['fold_id', 'cv_tag', 'html_id', 'sent_id', 'text', 'tag']]\n",
    "\n",
    "df_movie_reivews.to_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', index=False)\n",
    "df_movie_reivews = pd.read_csv(new_nltk_data + '/corpora/movie_reviews/movie_review.tsv', sep='\\t', \n",
    "                     dtype={'fold_id':int, 'cv_tag':str, 'html_id':str, 'sent_id':int,\n",
    "                            'text':str, 'tag':str})\n",
    "\n",
    "\n",
    "mr_bibtext = \"\"\"@InProceedings{Pang+Lee:04a,\n",
    "  author =       {Bo Pang and Lillian Lee},\n",
    "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
    "  booktitle =    \"Proceedings of the ACL\",\n",
    "  year =         2004\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "mr_meta = {'title': 'Sentiment Polarity Dataset Version 2.0',\n",
    "           'aka': 'Moview Review Data',\n",
    "           'source': 'http://www.cs.cornell.edu/people/pabo/movie-review-data/',\n",
    "           'authors': 'Bo Pang and Lillian Lee',\n",
    "           'license': 'Distributed with NLTK with permission from the authors.',\n",
    "            'bibtex':mr_bibtext}\n",
    "\n",
    "with open(new_nltk_data+'/corpora/movie_reviews/movie_reviews-meta.json', 'w') as fout:\n",
    "    json.dump(mr_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/webtext/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "rows = []\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/webtext/')):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    subcorp = filename.split('.')[0]\n",
    "    with open(old_nltk_data+'/corpora/webtext/' + filename, encoding='latin-1') as fin:\n",
    "        for line in fin:\n",
    "            rows.append({'text': line.strip(), 'domain': subcorp})\n",
    "df_webtext = pd.DataFrame(rows)[['text', 'domain']]\n",
    "\n",
    "\n",
    "df_webtext.to_csv(new_nltk_data +'/corpora/webtext/webtext.tsv', sep='\\t', index=False)\n",
    "df_webtext = pd.read_csv(new_nltk_data + '/corpora/webtext/webtext.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'domain':str})\n",
    "\n",
    "\n",
    "\n",
    "webtext_meta = {'title':'Web Text Corpus',\n",
    "                   'description': str(\"This is a collection of diverse, contemporary text genres, \"\n",
    "                                      \"collected by scraping publicly accessible archives of web postings. \"\n",
    "                                      \"This data is disseminated in preference to publishing URLs for \"\n",
    "                                       \"individuals to download and clean up (the usual model for web corpora).\"),\n",
    "                   \n",
    "                    'subcorpora': {'firefox': {'original_file': 'firefox.txt', \n",
    "                                               'description': 'Firefox support forum'},\n",
    "                                   'overheard': {'original_file': 'overheard.txt', \n",
    "                                               'description': 'Overheard in New York (partly censored)', \n",
    "                                                'source': 'http://www.overheardinnewyork.com/', \n",
    "                                                'year': '2006'},\n",
    "                                   'pirate': {'original_file': 'pirate.txt', \n",
    "                                               'description': \"Movie script from Pirates of the Caribbean: Dead Man's Chest\",\n",
    "                                                'source': 'http://www.overheardinnewyork.com/', \n",
    "                                                'year': '2006'},\n",
    "                                   'grail': {'original_file': 'grail.txt', \n",
    "                                               'description': 'Movie script from Monty Python and the Holy Grail',\n",
    "                                                'source': 'http://www.textfiles.com/media/SCRIPTS/grail', \n",
    "                                                'year': '2006'},\n",
    "                                   'singles': {'original_file': 'singles.txt', \n",
    "                                               'description': 'Singles ads',\n",
    "                                                'source': 'http://search.classifieds.news.com.au/',},\n",
    "                                   'wine': {'original_file': 'wine.txt', \n",
    "                                               'description': 'Fine Wine Diary',\n",
    "                                                'source': 'http://www.finewinediary.com/', \n",
    "                                                'year': '2005-6'},\n",
    "                                  }\n",
    "                    }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/webtext/webtext-meta.json', 'w') as fout:\n",
    "    json.dump(webtext_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/names/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "\n",
    "rows =[]\n",
    "for filename in sorted(os.listdir(old_nltk_data+'/corpora/names/')):\n",
    "    if not filename.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    label = filename.split('.')[0]\n",
    "    with open(old_nltk_data+'/corpora/names/' + filename) as fin:\n",
    "        for line in fin:\n",
    "            rows.append({'text': line.strip(), 'gender': label})\n",
    "        \n",
    "df_names = pd.DataFrame(rows)[['text', 'gender']]\n",
    "df_names.to_csv(new_nltk_data+'/corpora/names/names.tsv', sep='\\t', index=False)\n",
    "df_names = pd.read_csv(new_nltk_data+'/corpora/names/names.tsv', sep='\\t', \n",
    "                     dtype={'text':str, 'gender':str})\n",
    "            \n",
    "names_readme = \"\"\"Names Corpus, Version 1.3 (1994-03-29)\n",
    "Copyright (C) 1991 Mark Kantrowitz\n",
    "Additions by Bill Ross\n",
    "\n",
    "This corpus contains 5001 female names and 2943 male names, sorted\n",
    "alphabetically, one per line.\n",
    "\n",
    "You may use the lists of names for any purpose, so long as credit is\n",
    "given in any published work. You may also redistribute the list if you\n",
    "provide the recipients with a copy of this README file. The lists are\n",
    "not in the public domain (I retain the copyright on the lists) but are\n",
    "freely redistributable.  If you have any additions to the lists of\n",
    "names, I would appreciate receiving them.\n",
    "\n",
    "Mark Kantrowitz <mkant+@cs.cmu.edu>\n",
    "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\"\"\"\n",
    "    \n",
    "names_meta = {'title': 'Names Corpus',\n",
    "              'version': '1.3 (1994-03-29)',\n",
    "              'description': 'This corpus contains 5001 female names and 2943 male names',\n",
    "              'authors': 'Mark Kantrowitz, Bill Ross',\n",
    "              'license': 'Copyright (C) 1991 Mark Kantrowitz',\n",
    "              'source': 'http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/',\n",
    "              'readme': names_readme}\n",
    "\n",
    "\n",
    "with open(new_nltk_data+'/corpora/names/names-meta.json', 'w') as fout:\n",
    "    json.dump(names_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the Union\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "all_sotu = {}\n",
    "\n",
    "text_url = 'http://stateoftheunion.onetwothree.net/texts/'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "for li in BeautifulSoup(requests.get(text_url + 'index.html').content).find_all('li'):\n",
    "    if not li.find('a')['href']:\n",
    "        continue\n",
    "\n",
    "    sotu = text_url + li.find('a')['href']\n",
    "    if sotu.split('/')[-1].split('.')[0].isdigit():\n",
    "        year = li.find('a').text.split(', ')[-1]\n",
    "        if year in all_sotu:\n",
    "            continue\n",
    "        else:\n",
    "            soup = BeautifulSoup(requests.get(sotu, headers=headers).content)\n",
    "            name = soup.find('h2').text\n",
    "            date = soup.find('h3').text\n",
    "            year = date.split(', ')[1]\n",
    "            print(year, end=', ')\n",
    "            texts = [str(p) for p in soup.find_all('p')]\n",
    "            all_sotu[year] = {'year':year, 'date':date, 'name':name, 'texts':'\\n\\n'.join(texts).strip()}\n",
    "            \n",
    "for year in all_sotu:\n",
    "    lastname = all_sotu[year]['name'].split()[-1]\n",
    "    with open(f'sotu/{lastname}-{year}', 'w') as fout:\n",
    "        print(all_sotu[year]['texts'].strip(), file=fout)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = new_nltk_data+'/corpora/state_union/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "df_sotu = pd.read_csv('stateunion.tsv', sep='\\t', \n",
    "                     dtype={'texts':str, 'date':str, 'name':str},\n",
    "                     index_col=0)\n",
    "\n",
    "df_sotu.T['texts'] = ['\\n\\n'.join([deduplicate(' '.join(para.split('\\n')), ' ') \n",
    "                       for para in re.sub('<[^<]+?>', '', raw).strip().split('\\n\\n')])\n",
    "                      for raw in df_sotu.T.texts]\n",
    "\n",
    "sotu_meta = {'title': 'State of the Union: Addrresses', \n",
    "             'author': 'Brad Borevitz', \n",
    "             'source': 'http://stateoftheunion.onetwothree.net/texts/',\n",
    "             'year': '1790-2018'\n",
    "            }\n",
    "\n",
    "with open(new_nltk_data+'/corpora/state_union/state_union-meta.json', 'w') as fout:\n",
    "    json.dump(sotu_meta, fout, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    \n",
    "df_sotu.to_csv(new_nltk_data+'/corpora/state_union/state_union.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "      <th>1798</th>\n",
       "      <th>1799</th>\n",
       "      <th>...</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>January 8, 1790</td>\n",
       "      <td>October 25, 1791</td>\n",
       "      <td>November 6, 1792</td>\n",
       "      <td>December 3, 1793</td>\n",
       "      <td>November 19, 1794</td>\n",
       "      <td>December 8, 1795</td>\n",
       "      <td>December 7, 1796</td>\n",
       "      <td>November 22, 1797</td>\n",
       "      <td>December 8, 1798</td>\n",
       "      <td>December 3, 1799</td>\n",
       "      <td>...</td>\n",
       "      <td>February 24, 2009</td>\n",
       "      <td>January 27, 2010</td>\n",
       "      <td>January 25, 2011</td>\n",
       "      <td>January 24, 2012</td>\n",
       "      <td>February 12, 2013</td>\n",
       "      <td>January 28, 2014</td>\n",
       "      <td>January 20, 2015</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>February 28, 2017</td>\n",
       "      <td>January 30, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>...</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "      <td>Madame Speaker, Vice President Biden, Members ...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1790  \\\n",
       "date                                     January 8, 1790   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1791  \\\n",
       "date                                    October 25, 1791   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1792  \\\n",
       "date                                    November 6, 1792   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1793  \\\n",
       "date                                    December 3, 1793   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1794  \\\n",
       "date                                   November 19, 1794   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1795  \\\n",
       "date                                    December 8, 1795   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1796  \\\n",
       "date                                    December 7, 1796   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1797  \\\n",
       "date                                   November 22, 1797   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1798  \\\n",
       "date                                    December 8, 1798   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1799  \\\n",
       "date                                    December 3, 1799   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                             ...                          \\\n",
       "date                         ...                           \n",
       "name                         ...                           \n",
       "texts                        ...                           \n",
       "\n",
       "                                                    2009  \\\n",
       "date                                   February 24, 2009   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Mr. Vice President, Members of...   \n",
       "\n",
       "                                                    2010  \\\n",
       "date                                    January 27, 2010   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Vice President Biden, Members ...   \n",
       "\n",
       "                                                    2011  \\\n",
       "date                                    January 25, 2011   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2012  \\\n",
       "date                                    January 24, 2012   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2013  \\\n",
       "date                                   February 12, 2013   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2014  \\\n",
       "date                                    January 28, 2014   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2015  \\\n",
       "date                                    January 20, 2015   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2016  \\\n",
       "date                                    January 12, 2016   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2017  \\\n",
       "date                                   February 28, 2017   \n",
       "name                                     Donald J. Trump   \n",
       "texts  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "\n",
       "                                                    2018  \n",
       "date                                    January 30, 2018  \n",
       "name                                     Donald J. Trump  \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sotu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "      <th>1798</th>\n",
       "      <th>1799</th>\n",
       "      <th>...</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>January 8, 1790</td>\n",
       "      <td>October 25, 1791</td>\n",
       "      <td>November 6, 1792</td>\n",
       "      <td>December 3, 1793</td>\n",
       "      <td>November 19, 1794</td>\n",
       "      <td>December 8, 1795</td>\n",
       "      <td>December 7, 1796</td>\n",
       "      <td>November 22, 1797</td>\n",
       "      <td>December 8, 1798</td>\n",
       "      <td>December 3, 1799</td>\n",
       "      <td>...</td>\n",
       "      <td>February 24, 2009</td>\n",
       "      <td>January 27, 2010</td>\n",
       "      <td>January 25, 2011</td>\n",
       "      <td>January 24, 2012</td>\n",
       "      <td>February 12, 2013</td>\n",
       "      <td>January 28, 2014</td>\n",
       "      <td>January 20, 2015</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>February 28, 2017</td>\n",
       "      <td>January 30, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>...</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts</th>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Fellow-Citizens of the Senate and House of Rep...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>Gentlemen of the Senate and Gentlemen of the H...</td>\n",
       "      <td>...</td>\n",
       "      <td>Madame Speaker, Mr. Vice President, Members of...</td>\n",
       "      <td>Madame Speaker, Vice President Biden, Members ...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>Thank you very much. Mr. Speaker, Mr. Vice Pre...</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    1790  \\\n",
       "date                                     January 8, 1790   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1791  \\\n",
       "date                                    October 25, 1791   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1792  \\\n",
       "date                                    November 6, 1792   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1793  \\\n",
       "date                                    December 3, 1793   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1794  \\\n",
       "date                                   November 19, 1794   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1795  \\\n",
       "date                                    December 8, 1795   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1796  \\\n",
       "date                                    December 7, 1796   \n",
       "name                                   George Washington   \n",
       "texts  Fellow-Citizens of the Senate and House of Rep...   \n",
       "\n",
       "                                                    1797  \\\n",
       "date                                   November 22, 1797   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1798  \\\n",
       "date                                    December 8, 1798   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                                                    1799  \\\n",
       "date                                    December 3, 1799   \n",
       "name                                          John Adams   \n",
       "texts  Gentlemen of the Senate and Gentlemen of the H...   \n",
       "\n",
       "                             ...                          \\\n",
       "date                         ...                           \n",
       "name                         ...                           \n",
       "texts                        ...                           \n",
       "\n",
       "                                                    2009  \\\n",
       "date                                   February 24, 2009   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Mr. Vice President, Members of...   \n",
       "\n",
       "                                                    2010  \\\n",
       "date                                    January 27, 2010   \n",
       "name                                       Barack Obama    \n",
       "texts  Madame Speaker, Vice President Biden, Members ...   \n",
       "\n",
       "                                                    2011  \\\n",
       "date                                    January 25, 2011   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2012  \\\n",
       "date                                    January 24, 2012   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, members of Co...   \n",
       "\n",
       "                                                    2013  \\\n",
       "date                                   February 12, 2013   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2014  \\\n",
       "date                                    January 28, 2014   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2015  \\\n",
       "date                                    January 20, 2015   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2016  \\\n",
       "date                                    January 12, 2016   \n",
       "name                                       Barack Obama    \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...   \n",
       "\n",
       "                                                    2017  \\\n",
       "date                                   February 28, 2017   \n",
       "name                                     Donald J. Trump   \n",
       "texts  Thank you very much. Mr. Speaker, Mr. Vice Pre...   \n",
       "\n",
       "                                                    2018  \n",
       "date                                    January 30, 2018  \n",
       "name                                     Donald J. Trump  \n",
       "texts  Mr. Speaker, Mr. Vice President, Members of Co...  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('sotu'):\n",
    "    with open('sotu/'+filename) as fin, open('sotu-clean/'+filename, 'w') as fout:\n",
    "        fout.write(re.sub('<[^<]+?>', '', fin.read()).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City.db\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athens greece 1368\n",
      "bangkok thailand 1178\n",
      "barcelona spain 1280\n",
      "berlin east_germany 3481\n",
      "birmingham united_kingdom 1112\n",
      "bombay india 2839\n",
      "brussels belgium 986\n",
      "bucharest romania 1237\n",
      "budapest hungary 1757\n",
      "buenos_aires argentina 3404\n",
      "cairo egypt 2373\n",
      "calcutta india 2549\n",
      "canton china 1496\n",
      "caracas venezuela 488\n",
      "chicago united_states 3621\n",
      "chungking china 1100\n",
      "dairen china 544\n",
      "delhi india 1744\n",
      "detroit united_states 1850\n",
      "glasgow united_kingdom 1090\n",
      "hamburg west_germany 1700\n",
      "harbin china 760\n",
      "hongkong_city hongkong 2440\n",
      "hyderabad india 1086\n",
      "istanbul turkey 1215\n",
      "jakarta indonesia 533\n",
      "johannesburg south_africa 880\n",
      "karachi pakistan 1126\n",
      "kiev soviet_union 991\n",
      "kobe japan 765\n",
      "kowloon china 547\n",
      "kyoto japan 1204\n",
      "leningrad soviet_union 2800\n",
      "lima peru 835\n",
      "london united_kingdom 8346\n",
      "los_angeles united_states 1970\n",
      "madras india 1416\n",
      "madrid spain 1700\n",
      "manila philippines 1025\n",
      "melbourne australia 1595\n",
      "mexico_city mexico 3796\n",
      "milan italy 1269\n",
      "montreal canada 1109\n",
      "moscow soviet_union 4800\n",
      "mukden china 1551\n",
      "nagoya japan 1337\n",
      "nanking japan 1020\n",
      "naples italy 1012\n",
      "new_york united_states 7795\n",
      "osaka japan 2547\n",
      "paris france 2850\n",
      "peking china 2031\n",
      "philadelphia united_states 2072\n",
      "pusan south_korea 474\n",
      "rio_de_janeiro brazil 2413\n",
      "rome italy 1760\n",
      "saigon vietnam 695\n",
      "santiago chile 1350\n",
      "sao_paulo brazil 2228\n",
      "seoul south_korea 1446\n",
      "shanghai china 5407\n",
      "sian china 629\n",
      "singapore_city singapore 1264\n",
      "sydney australia 1898\n",
      "tehran iran 1010\n",
      "tientsin china 1795\n",
      "tokyo japan 8535\n",
      "toronto canada 668\n",
      "vienna austria 1766\n",
      "warsaw poland 965\n",
      "yokohama japan 1143\n"
     ]
    }
   ],
   "source": [
    "from nltk.sem.chat80 import cities2table, sql_query\n",
    "from sqlite3 import OperationalError\n",
    "try:\n",
    "    cities2table('cities.pl', 'city', 'city.db', verbose=True, setup=True)\n",
    "except OperationalError:\n",
    "    pass \n",
    "for row in sql_query('corpora/city_database/city.db', \"SELECT * FROM city_table\"):\n",
    "    city, country, population = row\n",
    "    print(city, country, population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('athens', 'greece', 1368)\n",
      "('bangkok', 'thailand', 1178)\n",
      "('barcelona', 'spain', 1280)\n",
      "('berlin', 'east_germany', 3481)\n",
      "('birmingham', 'united_kingdom', 1112)\n",
      "('bombay', 'india', 2839)\n",
      "('brussels', 'belgium', 986)\n",
      "('bucharest', 'romania', 1237)\n",
      "('budapest', 'hungary', 1757)\n",
      "('buenos_aires', 'argentina', 3404)\n",
      "('cairo', 'egypt', 2373)\n",
      "('calcutta', 'india', 2549)\n",
      "('canton', 'china', 1496)\n",
      "('caracas', 'venezuela', 488)\n",
      "('chicago', 'united_states', 3621)\n",
      "('chungking', 'china', 1100)\n",
      "('dairen', 'china', 544)\n",
      "('delhi', 'india', 1744)\n",
      "('detroit', 'united_states', 1850)\n",
      "('glasgow', 'united_kingdom', 1090)\n",
      "('hamburg', 'west_germany', 1700)\n",
      "('harbin', 'china', 760)\n",
      "('hongkong_city', 'hongkong', 2440)\n",
      "('hyderabad', 'india', 1086)\n",
      "('istanbul', 'turkey', 1215)\n",
      "('jakarta', 'indonesia', 533)\n",
      "('johannesburg', 'south_africa', 880)\n",
      "('karachi', 'pakistan', 1126)\n",
      "('kiev', 'soviet_union', 991)\n",
      "('kobe', 'japan', 765)\n",
      "('kowloon', 'china', 547)\n",
      "('kyoto', 'japan', 1204)\n",
      "('leningrad', 'soviet_union', 2800)\n",
      "('lima', 'peru', 835)\n",
      "('london', 'united_kingdom', 8346)\n",
      "('los_angeles', 'united_states', 1970)\n",
      "('madras', 'india', 1416)\n",
      "('madrid', 'spain', 1700)\n",
      "('manila', 'philippines', 1025)\n",
      "('melbourne', 'australia', 1595)\n",
      "('mexico_city', 'mexico', 3796)\n",
      "('milan', 'italy', 1269)\n",
      "('montreal', 'canada', 1109)\n",
      "('moscow', 'soviet_union', 4800)\n",
      "('mukden', 'china', 1551)\n",
      "('nagoya', 'japan', 1337)\n",
      "('nanking', 'japan', 1020)\n",
      "('naples', 'italy', 1012)\n",
      "('new_york', 'united_states', 7795)\n",
      "('osaka', 'japan', 2547)\n",
      "('paris', 'france', 2850)\n",
      "('peking', 'china', 2031)\n",
      "('philadelphia', 'united_states', 2072)\n",
      "('pusan', 'south_korea', 474)\n",
      "('rio_de_janeiro', 'brazil', 2413)\n",
      "('rome', 'italy', 1760)\n",
      "('saigon', 'vietnam', 695)\n",
      "('santiago', 'chile', 1350)\n",
      "('sao_paulo', 'brazil', 2228)\n",
      "('seoul', 'south_korea', 1446)\n",
      "('shanghai', 'china', 5407)\n",
      "('sian', 'china', 629)\n",
      "('singapore_city', 'singapore', 1264)\n",
      "('sydney', 'australia', 1898)\n",
      "('tehran', 'iran', 1010)\n",
      "('tientsin', 'china', 1795)\n",
      "('tokyo', 'japan', 8535)\n",
      "('toronto', 'canada', 668)\n",
      "('vienna', 'austria', 1766)\n",
      "('warsaw', 'poland', 965)\n",
      "('yokohama', 'japan', 1143)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
